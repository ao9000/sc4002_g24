{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Udr5hAt6AB4Z",
        "outputId": "082f84bb-7006-4d71-d590-7d894a6867d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.7/472.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.0.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UOlKHQ6uALOd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import pickle\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIWd8kb-AN3l",
        "outputId": "72a1072f-e5c9-4867-b501-d2fa67eff390"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "267\n"
          ]
        }
      ],
      "source": [
        "# Load the Rotten Tomatoes dataset\n",
        "dataset = load_dataset(\"rotten_tomatoes\")\n",
        "train_dataset = dataset['train']\n",
        "valid_dataset = dataset['validation']\n",
        "test_dataset = dataset['test']\n",
        "\n",
        "train_text = train_dataset.to_pandas()['text']\n",
        "\n",
        "max_text_len = 0\n",
        "for text in train_text:\n",
        "    max_text_len = max(max_text_len, len(text))\n",
        "\n",
        "print(max_text_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KbeQBRWAPZE",
        "outputId": "91d29f39-0d92-4e57-9c9e-a48ee7cb9ea7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'dict'>\n"
          ]
        }
      ],
      "source": [
        "# Load the embedding matrix and vocab from files\n",
        "with open('../embedding_matrix_300d.pkl', 'rb') as f:\n",
        "    embedding_matrix = pickle.load(f).astype(np.float32)\n",
        "    padding = [0 for i in range(300)]\n",
        "    embedding_matrix = np.insert(embedding_matrix, 0, padding, 0)\n",
        "    print(type(embedding_matrix))\n",
        "\n",
        "with open('../vocab_word_to_index_300d.pkl', 'rb') as f:\n",
        "    vocab_word_to_index = pickle.load(f)\n",
        "    print(type(vocab_word_to_index))\n",
        "\n",
        "# Convert to torch tensors\n",
        "embedding_matrix = torch.tensor(embedding_matrix)\n",
        "vocab_size, embedding_dim = embedding_matrix.shape\n",
        "#print(embedding_matrix[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tm5n5sLIB7L5",
        "outputId": "c69a7b25-384d-4b8f-ac7c-b09f840a142f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "4jZPWyvAARbG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import nltk\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "class SentimentDataset:\n",
        "    def __init__(self, dataset, word_to_index, max_len=30):\n",
        "        self.dataset = dataset\n",
        "        self.word_to_index = word_to_index\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.dataset[idx]['text']\n",
        "        label = self.dataset[idx]['label']\n",
        "\n",
        "        # Tokenization and word-to-index conversion\n",
        "        text = text.lower()\n",
        "        word_list = nltk.tokenize.word_tokenize(text)\n",
        "        word_list = [word.strip(\"'\\\"\") for word in word_list]\n",
        "        indices = [self.word_to_index.get(word, self.word_to_index.get('<UNK>')) + 1 for word in word_list]\n",
        "        indices = indices[:self.max_len] + [0] * (self.max_len - len(indices))  # Padding\n",
        "\n",
        "        return np.array(indices), np.array(label)\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        texts = []\n",
        "        labels = []\n",
        "        for i in range(len(self.dataset)):\n",
        "            features, label = self.__getitem__(i)\n",
        "            texts.append(features)\n",
        "            labels.append(label)\n",
        "        return np.array(texts), np.array(labels)\n",
        "\n",
        "train_texts, train_labels = SentimentDataset(train_dataset, vocab_word_to_index, max_len=30).preprocess_data()\n",
        "valid_texts, valid_labels = SentimentDataset(valid_dataset, vocab_word_to_index, max_len=30).preprocess_data()\n",
        "test_texts, test_labels = SentimentDataset(test_dataset, vocab_word_to_index, max_len=30).preprocess_data()\n",
        "\n",
        "# Convert preprocessed arrays to PyTorch tensors\n",
        "train_texts = torch.tensor(train_texts)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "valid_texts = torch.tensor(valid_texts)\n",
        "valid_labels = torch.tensor(valid_labels)\n",
        "test_texts = torch.tensor(test_texts)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "\n",
        "train_dataset = TensorDataset(train_texts, train_labels)\n",
        "valid_dataset = TensorDataset(valid_texts, valid_labels)\n",
        "test_dataset = TensorDataset(test_texts, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=128, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsJ4Vs1JAZLI"
      },
      "source": [
        "# Vanilla RNN - Max Pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lVd4Cpd3ASDY"
      },
      "outputs": [],
      "source": [
        "## Max Pooling##\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "class SentimentRNN_MaxPool(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim, output_dim, num_layers = 1, freeze_embeddings=True, dropout = 0.5):\n",
        "        super(SentimentRNN_MaxPool, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=freeze_embeddings)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True, device= device)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim, device= device)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        out, hidden = self.rnn(embedded)\n",
        "        out = out.max(dim=1).values\n",
        "        #out = self.dropout(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "# Model hyperparameters\n",
        "hidden_dim = 128\n",
        "output_dim = 1  # Binary classification (positive, negative)\n",
        "\n",
        "model_maxp = SentimentRNN_MaxPool(embedding_matrix, hidden_dim, output_dim, 1)\n",
        "#print(model.embedding.weight[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1BioAu81AlGA"
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train_model(model, train_loader, valid_loader, epochs=30):\n",
        "    global best_metric, no_improvement_count #variables for early stopping\n",
        "    for epoch in range(epochs):\n",
        "        # Set model to training mode\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        total_train_correct = 0\n",
        "        total_train_samples = 0\n",
        "\n",
        "        for texts, labels in train_loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # Reset gradients\n",
        "\n",
        "            # Forward pass: get predictions\n",
        "            predictions = model(texts)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(predictions, labels.unsqueeze(1).float())\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            # Backpropagation and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accuracy calculation\n",
        "            predicted_labels = (predictions > 0.5).int()\n",
        "            total_train_correct += (predicted_labels.squeeze() == labels).sum().item()\n",
        "\n",
        "            #print(total_train_correct)\n",
        "            total_train_samples += labels.size(0)\n",
        "\n",
        "        # Calculate and print average training accuracy and loss per epoch\n",
        "        train_accuracy = total_train_correct / total_train_samples\n",
        "        train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # Evaluate model on validation set\n",
        "        valid_accuracy = evaluate_model(model, valid_loader)\n",
        "\n",
        "        print(f'Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f} | Validation Accuracy: {valid_accuracy:.4f}')\n",
        "\n",
        "        # Early stopping check\n",
        "        if best_metric is None or valid_accuracy > best_metric:\n",
        "            best_metric = valid_accuracy\n",
        "            no_improvement_count = 0  # Reset counter\n",
        "            torch.save(model.state_dict(), 'best_max_model.pt')  # Save best model state\n",
        "        else:\n",
        "            no_improvement_count += 1  # Increment counter if no improvement\n",
        "\n",
        "        if no_improvement_count >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            break  # Exit training loop if no improvement for `patience` epochs\n",
        "\n",
        "def evaluate_model(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            predictions = model(texts)\n",
        "            predicted_labels = (predictions >= 0.5).int()\n",
        "            correct += (predicted_labels.squeeze() == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "yyoFBqnFAeuM"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model_maxp.parameters(), lr=0.0001)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "model_maxp.to(device)\n",
        "\n",
        "patience = 5\n",
        "best_metric = None\n",
        "no_improvement_count = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "DPUKmYbrAn70"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Ensure deterministic behavior for cuDNN (CUDA)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vc9JNVQ7AqWN",
        "outputId": "5bd4623c-85b0-4fc8-d61e-55797abde5ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train Loss: 0.6888 | Train Accuracy: 0.5489 | Validation Accuracy: 0.6313\n",
            "Epoch 2 | Train Loss: 0.6678 | Train Accuracy: 0.6613 | Validation Accuracy: 0.6942\n",
            "Epoch 3 | Train Loss: 0.6100 | Train Accuracy: 0.6892 | Validation Accuracy: 0.6886\n",
            "Epoch 4 | Train Loss: 0.5685 | Train Accuracy: 0.7127 | Validation Accuracy: 0.7111\n",
            "Epoch 5 | Train Loss: 0.5418 | Train Accuracy: 0.7313 | Validation Accuracy: 0.7308\n",
            "Epoch 6 | Train Loss: 0.5239 | Train Accuracy: 0.7420 | Validation Accuracy: 0.7439\n",
            "Epoch 7 | Train Loss: 0.5111 | Train Accuracy: 0.7512 | Validation Accuracy: 0.7467\n",
            "Epoch 8 | Train Loss: 0.5004 | Train Accuracy: 0.7544 | Validation Accuracy: 0.7523\n",
            "Epoch 9 | Train Loss: 0.4918 | Train Accuracy: 0.7623 | Validation Accuracy: 0.7411\n",
            "Epoch 10 | Train Loss: 0.4890 | Train Accuracy: 0.7644 | Validation Accuracy: 0.7439\n",
            "Epoch 11 | Train Loss: 0.4851 | Train Accuracy: 0.7675 | Validation Accuracy: 0.7523\n",
            "Epoch 12 | Train Loss: 0.4816 | Train Accuracy: 0.7680 | Validation Accuracy: 0.7505\n",
            "Epoch 13 | Train Loss: 0.4786 | Train Accuracy: 0.7675 | Validation Accuracy: 0.7486\n",
            "Early stopping triggered after 13 epochs\n"
          ]
        }
      ],
      "source": [
        "train_model(model_maxp, train_loader, valid_loader, epochs=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyCRvvmQAr6R",
        "outputId": "650a6fa2-bac8-47de-d162-62884778e6d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.7486\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-35-933d99f60023>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_maxp.load_state_dict(torch.load('best_max_model.pt'))\n"
          ]
        }
      ],
      "source": [
        "# Load the best model\n",
        "model_maxp.load_state_dict(torch.load('best_max_model.pt'))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_acc = evaluate_model(model_maxp, test_loader)\n",
        "print(f'Test Accuracy: {test_acc:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIgtR_h8ClC5"
      },
      "source": [
        "# Vanilla RNN - Mean Pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "NxMN698oCt6J"
      },
      "outputs": [],
      "source": [
        "## Max Pooling##\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "class SentimentRNN_MeanPool(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim, output_dim, num_layers = 1, freeze_embeddings=True, dropout = 0.5):\n",
        "        super(SentimentRNN_MeanPool, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=freeze_embeddings)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True, device= device)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim, device= device)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        out, hidden = self.rnn(embedded)\n",
        "        out = out.mean(dim=1)\n",
        "        #out = self.dropout(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "# Model hyperparameters\n",
        "hidden_dim = 128\n",
        "output_dim = 1  # Binary classification (positive, negative)\n",
        "\n",
        "model_meanp = SentimentRNN_MeanPool(embedding_matrix, hidden_dim, output_dim, 1)\n",
        "#print(model.embedding.weight[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "74Ra9nNmDSZQ"
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train_model(model, train_loader, valid_loader, epochs=30):\n",
        "    global best_metric, no_improvement_count #variables for early stopping\n",
        "    for epoch in range(epochs):\n",
        "        # Set model to training mode\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        total_train_correct = 0\n",
        "        total_train_samples = 0\n",
        "\n",
        "        for texts, labels in train_loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # Reset gradients\n",
        "\n",
        "            # Forward pass: get predictions\n",
        "            predictions = model(texts)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(predictions, labels.unsqueeze(1).float())\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            # Backpropagation and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accuracy calculation\n",
        "            predicted_labels = (predictions > 0.5).int()\n",
        "            total_train_correct += (predicted_labels.squeeze() == labels).sum().item()\n",
        "\n",
        "            #print(total_train_correct)\n",
        "            total_train_samples += labels.size(0)\n",
        "\n",
        "        # Calculate and print average training accuracy and loss per epoch\n",
        "        train_accuracy = total_train_correct / total_train_samples\n",
        "        train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # Evaluate model on validation set\n",
        "        valid_accuracy = evaluate_model(model, valid_loader)\n",
        "\n",
        "        print(f'Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f} | Validation Accuracy: {valid_accuracy:.4f}')\n",
        "\n",
        "        # Early stopping check\n",
        "        if best_metric is None or valid_accuracy > best_metric:\n",
        "            best_metric = valid_accuracy\n",
        "            no_improvement_count = 0  # Reset counter\n",
        "            torch.save(model.state_dict(), 'best_mean_model.pt')  # Save best model state\n",
        "        else:\n",
        "            no_improvement_count += 1  # Increment counter if no improvement\n",
        "\n",
        "        if no_improvement_count >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            break  # Exit training loop if no improvement for `patience` epochs\n",
        "\n",
        "def evaluate_model(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            predictions = model(texts)\n",
        "            predicted_labels = (predictions >= 0.5).int()\n",
        "            correct += (predicted_labels.squeeze() == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "e_yc0eQqCzJT"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model_meanp.parameters(), lr=0.0001)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "model_meanp.to(device)\n",
        "\n",
        "patience = 5\n",
        "best_metric = None\n",
        "no_improvement_count = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "fGSOt2HLC_tx"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Ensure deterministic behavior for cuDNN (CUDA)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LdaFmQkDB2t",
        "outputId": "f131b6a5-dbff-44fc-d6e1-c1c87d38d4cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train Loss: 0.6864 | Train Accuracy: 0.5994 | Validation Accuracy: 0.6886\n",
            "Epoch 2 | Train Loss: 0.6143 | Train Accuracy: 0.6819 | Validation Accuracy: 0.7045\n",
            "Epoch 3 | Train Loss: 0.5549 | Train Accuracy: 0.7177 | Validation Accuracy: 0.7195\n",
            "Epoch 4 | Train Loss: 0.5342 | Train Accuracy: 0.7332 | Validation Accuracy: 0.7223\n",
            "Epoch 5 | Train Loss: 0.5200 | Train Accuracy: 0.7433 | Validation Accuracy: 0.7280\n",
            "Epoch 6 | Train Loss: 0.5101 | Train Accuracy: 0.7503 | Validation Accuracy: 0.7383\n",
            "Epoch 7 | Train Loss: 0.5046 | Train Accuracy: 0.7546 | Validation Accuracy: 0.7326\n",
            "Epoch 8 | Train Loss: 0.5008 | Train Accuracy: 0.7526 | Validation Accuracy: 0.7411\n",
            "Epoch 9 | Train Loss: 0.4947 | Train Accuracy: 0.7571 | Validation Accuracy: 0.7345\n",
            "Epoch 10 | Train Loss: 0.4953 | Train Accuracy: 0.7620 | Validation Accuracy: 0.7270\n",
            "Epoch 11 | Train Loss: 0.4916 | Train Accuracy: 0.7608 | Validation Accuracy: 0.7486\n",
            "Epoch 12 | Train Loss: 0.4906 | Train Accuracy: 0.7598 | Validation Accuracy: 0.7477\n",
            "Epoch 13 | Train Loss: 0.4877 | Train Accuracy: 0.7645 | Validation Accuracy: 0.7458\n",
            "Epoch 14 | Train Loss: 0.4855 | Train Accuracy: 0.7623 | Validation Accuracy: 0.7495\n",
            "Epoch 15 | Train Loss: 0.4831 | Train Accuracy: 0.7655 | Validation Accuracy: 0.7467\n",
            "Epoch 16 | Train Loss: 0.4829 | Train Accuracy: 0.7665 | Validation Accuracy: 0.7486\n",
            "Epoch 17 | Train Loss: 0.4784 | Train Accuracy: 0.7662 | Validation Accuracy: 0.7514\n",
            "Epoch 18 | Train Loss: 0.4799 | Train Accuracy: 0.7686 | Validation Accuracy: 0.7514\n",
            "Epoch 19 | Train Loss: 0.4792 | Train Accuracy: 0.7686 | Validation Accuracy: 0.7505\n",
            "Epoch 20 | Train Loss: 0.4771 | Train Accuracy: 0.7688 | Validation Accuracy: 0.7533\n",
            "Epoch 21 | Train Loss: 0.4785 | Train Accuracy: 0.7675 | Validation Accuracy: 0.7467\n",
            "Epoch 22 | Train Loss: 0.4766 | Train Accuracy: 0.7714 | Validation Accuracy: 0.7439\n",
            "Epoch 23 | Train Loss: 0.4732 | Train Accuracy: 0.7688 | Validation Accuracy: 0.7486\n",
            "Epoch 24 | Train Loss: 0.4750 | Train Accuracy: 0.7682 | Validation Accuracy: 0.7477\n",
            "Epoch 25 | Train Loss: 0.4725 | Train Accuracy: 0.7699 | Validation Accuracy: 0.7458\n",
            "Early stopping triggered after 25 epochs\n"
          ]
        }
      ],
      "source": [
        "train_model(model_meanp, train_loader, valid_loader, epochs=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZn42vnnDDyr",
        "outputId": "8981342d-a2f6-4273-e94e-02e4cb431e50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.7411\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-34-cebbdb159473>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_maxp.load_state_dict(torch.load('best_mean_model.pt'))\n"
          ]
        }
      ],
      "source": [
        "# Load the best model\n",
        "model_maxp.load_state_dict(torch.load('best_mean_model.pt'))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_acc = evaluate_model(model_maxp, test_loader)\n",
        "print(f'Test Accuracy: {test_acc:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci4DnSSID7sU"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFYlUkA2EQyL"
      },
      "source": [
        "# Vanilla RNN - Concatenation Pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "VOgr3y1cE24X"
      },
      "outputs": [],
      "source": [
        "## Mixed Pooling##\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "class SentimentRNN_MixedPool(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim, output_dim, num_layers = 1, freeze_embeddings=True, dropout = 0.5):\n",
        "        super(SentimentRNN_MixedPool, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=freeze_embeddings)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True, device= device)\n",
        "        self.fc = nn.Linear(3*hidden_dim, output_dim, device= device)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        out, hidden = self.rnn(embedded)\n",
        "\n",
        "        last_hidden = out[:, -1, :]  #last hidden state\n",
        "        mean_pooled = out.mean(dim=1) #max pooling\n",
        "        max_pooled = out.max(dim=1).values #average pooling\n",
        "\n",
        "        out = torch.cat([last_hidden, mean_pooled, max_pooled], dim=1)\n",
        "        #out = self.dropout(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "# Model hyperparameters\n",
        "hidden_dim = 128\n",
        "output_dim = 1  # Binary classification (positive, negative)\n",
        "\n",
        "model_mixedp = SentimentRNN_MixedPool(embedding_matrix, hidden_dim, output_dim, 1)\n",
        "#print(model.embedding.weight[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "DRcgQumnE4dr"
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train_model(model, train_loader, valid_loader, epochs=30):\n",
        "    global best_metric, no_improvement_count #variables for early stopping\n",
        "    for epoch in range(epochs):\n",
        "        # Set model to training mode\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        total_train_correct = 0\n",
        "        total_train_samples = 0\n",
        "\n",
        "        for texts, labels in train_loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # Reset gradients\n",
        "\n",
        "            # Forward pass: get predictions\n",
        "            predictions = model(texts)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(predictions, labels.unsqueeze(1).float())\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            # Backpropagation and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accuracy calculation\n",
        "            predicted_labels = (predictions > 0.5).int()\n",
        "            total_train_correct += (predicted_labels.squeeze() == labels).sum().item()\n",
        "\n",
        "            #print(total_train_correct)\n",
        "            total_train_samples += labels.size(0)\n",
        "\n",
        "        # Calculate and print average training accuracy and loss per epoch\n",
        "        train_accuracy = total_train_correct / total_train_samples\n",
        "        train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # Evaluate model on validation set\n",
        "        valid_accuracy = evaluate_model(model, valid_loader)\n",
        "\n",
        "        print(f'Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f} | Validation Accuracy: {valid_accuracy:.4f}')\n",
        "\n",
        "        # Early stopping check\n",
        "        if best_metric is None or valid_accuracy > best_metric:\n",
        "            best_metric = valid_accuracy\n",
        "            no_improvement_count = 0  # Reset counter\n",
        "            torch.save(model.state_dict(), 'best_mixed_model.pt')  # Save best model state\n",
        "        else:\n",
        "            no_improvement_count += 1  # Increment counter if no improvement\n",
        "\n",
        "        if no_improvement_count >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            break  # Exit training loop if no improvement for `patience` epochs\n",
        "\n",
        "def evaluate_model(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            predictions = model(texts)\n",
        "            predicted_labels = (predictions >= 0.5).int()\n",
        "            correct += (predicted_labels.squeeze() == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "oWFSpZVcE6CO"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model_mixedp.parameters(), lr=0.0001)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "model_mixedp.to(device)\n",
        "\n",
        "patience = 5\n",
        "best_metric = None\n",
        "no_improvement_count = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "vhYQy6s4E8Sr"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Ensure deterministic behavior for cuDNN (CUDA)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ri_qI-EAE-Ma",
        "outputId": "e45c6fc9-dd33-49d8-8468-66b3dbcce6df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train Loss: 0.6902 | Train Accuracy: 0.5579 | Validation Accuracy: 0.6088\n",
            "Epoch 2 | Train Loss: 0.6707 | Train Accuracy: 0.6797 | Validation Accuracy: 0.6876\n",
            "Epoch 3 | Train Loss: 0.5833 | Train Accuracy: 0.7019 | Validation Accuracy: 0.7148\n",
            "Epoch 4 | Train Loss: 0.5300 | Train Accuracy: 0.7389 | Validation Accuracy: 0.7223\n",
            "Epoch 5 | Train Loss: 0.5086 | Train Accuracy: 0.7518 | Validation Accuracy: 0.7467\n",
            "Epoch 6 | Train Loss: 0.4972 | Train Accuracy: 0.7597 | Validation Accuracy: 0.7392\n",
            "Epoch 7 | Train Loss: 0.4907 | Train Accuracy: 0.7627 | Validation Accuracy: 0.7336\n",
            "Epoch 8 | Train Loss: 0.4870 | Train Accuracy: 0.7638 | Validation Accuracy: 0.7533\n",
            "Epoch 9 | Train Loss: 0.4812 | Train Accuracy: 0.7695 | Validation Accuracy: 0.7439\n",
            "Epoch 10 | Train Loss: 0.4799 | Train Accuracy: 0.7715 | Validation Accuracy: 0.7514\n",
            "Epoch 11 | Train Loss: 0.4794 | Train Accuracy: 0.7716 | Validation Accuracy: 0.7533\n",
            "Epoch 12 | Train Loss: 0.4782 | Train Accuracy: 0.7721 | Validation Accuracy: 0.7495\n",
            "Epoch 13 | Train Loss: 0.4744 | Train Accuracy: 0.7730 | Validation Accuracy: 0.7495\n",
            "Early stopping triggered after 13 epochs\n"
          ]
        }
      ],
      "source": [
        "train_model(model_mixedp, train_loader, valid_loader, epochs=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSFnur31E_eM",
        "outputId": "15c5f484-7b5a-43f1-d119-4be841885713"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.7505\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-58-277dceb17da0>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_mixedp.load_state_dict(torch.load('best_mixed_model.pt'))\n"
          ]
        }
      ],
      "source": [
        "# Load the best model\n",
        "model_mixedp.load_state_dict(torch.load('best_mixed_model.pt'))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_acc = evaluate_model(model_mixedp, test_loader)\n",
        "print(f'Test Accuracy: {test_acc:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ph4zKcaG_Wj"
      },
      "source": [
        "# Vanilla RNN - Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "ze0aKTNXHqkO"
      },
      "outputs": [],
      "source": [
        "## Mixed Pooling##\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "class SentimentRNN_Attention(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim, output_dim, num_layers = 1, freeze_embeddings=True, dropout = 0.5):\n",
        "        super(SentimentRNN_Attention, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=freeze_embeddings)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True, device= device)\n",
        "        self.attention = nn.Linear(hidden_dim, 1, bias=False)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim, device= device)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        out, hidden = self.rnn(embedded)\n",
        "        attention_weights = torch.softmax(self.attention(out), dim=1)\n",
        "        out = torch.sum(out * attention_weights, dim=1)\n",
        "\n",
        "        out = self.fc(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "# Model hyperparameters\n",
        "hidden_dim = 128\n",
        "output_dim = 1  # Binary classification (positive, negative)\n",
        "\n",
        "model_attention= SentimentRNN_Attention(embedding_matrix, hidden_dim, output_dim, 1)\n",
        "#print(model.embedding.weight[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "cs3w2d9_Hufz"
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train_model(model, train_loader, valid_loader, epochs=30):\n",
        "    global best_metric, no_improvement_count #variables for early stopping\n",
        "    for epoch in range(epochs):\n",
        "        # Set model to training mode\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        total_train_correct = 0\n",
        "        total_train_samples = 0\n",
        "\n",
        "        for texts, labels in train_loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # Reset gradients\n",
        "\n",
        "            # Forward pass: get predictions\n",
        "            predictions = model(texts)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(predictions, labels.unsqueeze(1).float())\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            # Backpropagation and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accuracy calculation\n",
        "            predicted_labels = (predictions > 0.5).int()\n",
        "            total_train_correct += (predicted_labels.squeeze() == labels).sum().item()\n",
        "\n",
        "            #print(total_train_correct)\n",
        "            total_train_samples += labels.size(0)\n",
        "\n",
        "        # Calculate and print average training accuracy and loss per epoch\n",
        "        train_accuracy = total_train_correct / total_train_samples\n",
        "        train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # Evaluate model on validation set\n",
        "        valid_accuracy = evaluate_model(model, valid_loader)\n",
        "\n",
        "        print(f'Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f} | Validation Accuracy: {valid_accuracy:.4f}')\n",
        "\n",
        "        # Early stopping check\n",
        "        if best_metric is None or valid_accuracy > best_metric:\n",
        "            best_metric = valid_accuracy\n",
        "            no_improvement_count = 0  # Reset counter\n",
        "            torch.save(model.state_dict(), 'best_attention_model.pt')  # Save best model state\n",
        "        else:\n",
        "            no_improvement_count += 1  # Increment counter if no improvement\n",
        "\n",
        "        if no_improvement_count >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            break  # Exit training loop if no improvement for `patience` epochs\n",
        "\n",
        "def evaluate_model(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            predictions = model(texts)\n",
        "            predicted_labels = (predictions >= 0.5).int()\n",
        "            correct += (predicted_labels.squeeze() == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "6iSxDKCYHwN5"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model_attention.parameters(), lr=0.0001)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "model_attention.to(device)\n",
        "\n",
        "patience = 5\n",
        "best_metric = None\n",
        "no_improvement_count = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "zq0OyjOHIKHa"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Ensure deterministic behavior for cuDNN (CUDA)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2le7MeCGIL3T",
        "outputId": "0a14a482-7f26-4b23-c2a8-33434b44f088"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train Loss: 0.6859 | Train Accuracy: 0.6035 | Validation Accuracy: 0.6979\n",
            "Epoch 2 | Train Loss: 0.6127 | Train Accuracy: 0.6864 | Validation Accuracy: 0.7073\n",
            "Epoch 3 | Train Loss: 0.5521 | Train Accuracy: 0.7183 | Validation Accuracy: 0.7214\n",
            "Epoch 4 | Train Loss: 0.5317 | Train Accuracy: 0.7338 | Validation Accuracy: 0.7205\n",
            "Epoch 5 | Train Loss: 0.5170 | Train Accuracy: 0.7455 | Validation Accuracy: 0.7233\n",
            "Epoch 6 | Train Loss: 0.5067 | Train Accuracy: 0.7523 | Validation Accuracy: 0.7486\n",
            "Epoch 7 | Train Loss: 0.5013 | Train Accuracy: 0.7546 | Validation Accuracy: 0.7326\n",
            "Epoch 8 | Train Loss: 0.4972 | Train Accuracy: 0.7553 | Validation Accuracy: 0.7505\n",
            "Epoch 9 | Train Loss: 0.4918 | Train Accuracy: 0.7607 | Validation Accuracy: 0.7345\n",
            "Epoch 10 | Train Loss: 0.4925 | Train Accuracy: 0.7625 | Validation Accuracy: 0.7317\n",
            "Epoch 11 | Train Loss: 0.4898 | Train Accuracy: 0.7612 | Validation Accuracy: 0.7495\n",
            "Epoch 12 | Train Loss: 0.4876 | Train Accuracy: 0.7625 | Validation Accuracy: 0.7477\n",
            "Epoch 13 | Train Loss: 0.4850 | Train Accuracy: 0.7627 | Validation Accuracy: 0.7448\n",
            "Early stopping triggered after 13 epochs\n"
          ]
        }
      ],
      "source": [
        "train_model(model_attention, train_loader, valid_loader, epochs=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRp_qtVuIOBR",
        "outputId": "3dcd2d87-8082-4c55-953c-098a1d1c1e6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.7495\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-69-c59b817bf65d>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_attention.load_state_dict(torch.load('best_attention_model.pt'))\n"
          ]
        }
      ],
      "source": [
        "# Load the best model\n",
        "model_attention.load_state_dict(torch.load('best_attention_model.pt'))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_acc = evaluate_model(model_attention, test_loader)\n",
        "print(f'Test Accuracy: {test_acc:.4f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPjrEm5ryXPCYj7kkhXJjL9",
      "collapsed_sections": [
        "xsJ4Vs1JAZLI",
        "rIgtR_h8ClC5",
        "UFYlUkA2EQyL",
        "6Ph4zKcaG_Wj"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
