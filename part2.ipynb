{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267\n"
     ]
    }
   ],
   "source": [
    "# Load the Rotten Tomatoes dataset\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train']\n",
    "valid_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "train_text = train_dataset.to_pandas()['text']\n",
    "\n",
    "max_text_len = 0\n",
    "for text in train_text:\n",
    "    max_text_len = max(max_text_len, len(text))\n",
    "\n",
    "print(max_text_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "tensor([[-0.4901, -0.2097,  0.1249,  ..., -0.0770, -0.2399, -1.4470],\n",
      "        [ 0.0151,  0.2401, -0.1368,  ...,  0.0424, -0.0365, -0.0043],\n",
      "        [ 0.2482,  0.5113,  0.1887,  ...,  0.0801,  0.2962,  0.4851],\n",
      "        ...,\n",
      "        [-0.1226,  0.3450, -0.8120,  ...,  0.6396, -0.3635, -0.4805],\n",
      "        [ 0.1039, -1.2005, -0.3010,  ..., -0.4107,  0.7417,  0.5984],\n",
      "        [ 0.0151,  0.2401, -0.1368,  ...,  0.0424, -0.0365, -0.0043]])\n"
     ]
    }
   ],
   "source": [
    "# Load the embedding matrix and vocab from files\n",
    "with open('embedding_matrix.pkl', 'rb') as f:\n",
    "    embedding_matrix = pickle.load(f).astype(np.float32)\n",
    "    padding = [0 for i in range(50)]\n",
    "    embedding_matrix.append(padding)\n",
    "    print(type(embedding_matrix))\n",
    "\n",
    "with open('vocab_word_to_index.pkl', 'rb') as f:\n",
    "    vocab_word_to_index = pickle.load(f)\n",
    "    print(type(vocab_word_to_index))\n",
    "\n",
    "# Convert to torch tensors\n",
    "embedding_matrix = torch.tensor(embedding_matrix)\n",
    "vocab_size, embedding_dim = embedding_matrix.shape\n",
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, dataset, word_to_index, max_len=max_text_len):\n",
    "        self.dataset = dataset\n",
    "        self.word_to_index = word_to_index\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[idx]['text']\n",
    "        label = self.dataset[idx]['label']\n",
    "        \n",
    "        # Convert words to indices\n",
    "        \n",
    "        text = text.lower()\n",
    "        #tokenise words\n",
    "        word_list = nltk.tokenize.word_tokenize(text)\n",
    "        word_list = [word.strip(\"'\\\"\") for word in word_list]\n",
    "        tokens = set()\n",
    "        tokens.update(word_list)\n",
    "        tokens.discard('')\n",
    "        indices = [self.word_to_index.get(word, self.word_to_index.get('<UNK>')) for word in tokens]\n",
    "        indices = indices[:self.max_len] + [0] * (self.max_len - len(indices))  # Padding\n",
    "        \n",
    "        return torch.tensor(indices), torch.tensor(label)\n",
    "\n",
    "train_data = SentimentDataset(train_dataset, vocab_word_to_index)\n",
    "valid_data = SentimentDataset(valid_dataset, vocab_word_to_index)\n",
    "test_data = SentimentDataset(test_dataset, vocab_word_to_index)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4901, -0.2097,  0.1249, -0.3952, -0.4148,  0.7488,  0.3754,  0.3650,\n",
      "        -0.0177, -0.3884,  0.1328, -0.3996, -0.6031,  0.6804, -0.5267,  1.0355,\n",
      "         0.8665,  0.2221,  0.6303, -0.8495,  0.6170, -0.0247,  0.8059, -0.1039,\n",
      "         0.1430,  0.2291, -0.7631,  1.6906,  1.1369, -0.7731,  0.7997, -0.0726,\n",
      "         1.0869,  0.1207, -0.0339,  0.8330, -0.3656,  0.5224,  0.5808,  0.4711,\n",
      "        -0.2782, -0.5437,  0.3406,  0.4131, -0.2171, -0.4031,  0.5080, -0.0770,\n",
      "        -0.2399, -1.4470])\n"
     ]
    }
   ],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, num_layers = 1, freeze_embeddings=True):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=freeze_embeddings)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        out, hidden = self.rnn(embedded)\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Using the last hidden state for classification\n",
    "        return self.fc(out)\n",
    "\n",
    "# Model hyperparameters\n",
    "hidden_dim = 128\n",
    "output_dim = 2  # Binary classification (positive, negative)\n",
    "\n",
    "model = SentimentRNN(embedding_matrix, hidden_dim, output_dim, 2)\n",
    "print(model.embedding.weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epoch_loss = []\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, valid_loader, epochs=10):\n",
    "    best_valid_acc = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        for texts, labels in train_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(texts)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += (predictions.argmax(1) == labels).sum().item()\n",
    "        \n",
    "        valid_acc = evaluate_model(model, valid_loader)\n",
    "        print(f'Epoch {epoch+1} | Train Loss: {epoch_loss / len(train_loader):.4f} | Validation Accuracy: {valid_acc:.4f}')\n",
    "        \n",
    "        if valid_acc > best_valid_acc:\n",
    "            best_valid_acc = valid_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            predictions = model(texts)\n",
    "            correct += (predictions.argmax(1) == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.6992 | Validation Accuracy: 0.5038\n",
      "Epoch 2 | Train Loss: 0.6980 | Validation Accuracy: 0.4962\n",
      "Epoch 3 | Train Loss: 0.6993 | Validation Accuracy: 0.4887\n",
      "Epoch 4 | Train Loss: 0.6966 | Validation Accuracy: 0.4925\n",
      "Epoch 5 | Train Loss: 0.6965 | Validation Accuracy: 0.5000\n",
      "Epoch 6 | Train Loss: 0.6968 | Validation Accuracy: 0.4906\n",
      "Epoch 7 | Train Loss: 0.6976 | Validation Accuracy: 0.5075\n",
      "Epoch 8 | Train Loss: 0.6996 | Validation Accuracy: 0.4841\n",
      "Epoch 9 | Train Loss: 0.6963 | Validation Accuracy: 0.5000\n",
      "Epoch 10 | Train Loss: 0.6967 | Validation Accuracy: 0.4822\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model, train_loader, valid_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5028\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_acc = evaluate_model(model, test_loader)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
