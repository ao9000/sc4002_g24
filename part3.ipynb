{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train'].to_pandas()\n",
    "validation_dataset = dataset['validation'].to_pandas()\n",
    "test_dataset = dataset['test'].to_pandas()\n",
    "max_len=max(0,train_dataset[\"text\"].apply(lambda x:len(x)).max())\n",
    "max_len=max(max_len,validation_dataset[\"text\"].apply(lambda x:len(x)).max())\n",
    "max_len=max(max_len,test_dataset[\"text\"].apply(lambda x:len(x)).max())\n",
    "max_len+=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def prep_pretrained_embedding():\n",
    "    def build_vocab(train_dataset):\n",
    "        # Create set, unique words only\n",
    "        vocab = set()\n",
    "        train_dataset_pos = []\n",
    "        \n",
    "        # Loop thru each sentence in training dataset\n",
    "        for sentence in train_dataset['text']:\n",
    "            # Basic text processing\n",
    "            \n",
    "            # Case folding\n",
    "            sentence = sentence.lower()\n",
    "            \n",
    "            # NLTK tokenizer does a good job at separating meaningful words + punctuations\n",
    "            # Better than defining regex ourselves\n",
    "            word_list = nltk.tokenize.word_tokenize(sentence)\n",
    "            \n",
    "            # # Further split words into separate words\n",
    "            # # e.g., 'well-being' -> 'well', 'being'\n",
    "            # # e.g., 'music/song' -> 'music', 'song'\n",
    "            # split_word_list = []\n",
    "            # for word in sentence_list:\n",
    "            #     split_word_list.extend(word.replace('-', ' ').replace('/', ' ').split())\n",
    "            \n",
    "            # Dont remove all special characters, some are meaningful\n",
    "            # Some words are surrounded by single/double quotes\n",
    "            word_list = [word.strip(\"'\\\"\") for word in word_list]\n",
    "            \n",
    "            # Add into set\n",
    "            vocab.update(word_list)\n",
    "            \n",
    "            # Get pos tags\n",
    "            # Also build POS tags\n",
    "            pos_tags = nltk.pos_tag(word_list)\n",
    "            train_dataset_pos.append(pos_tags)\n",
    "            \n",
    "        vocab.discard('')\n",
    "        return vocab, train_dataset_pos\n",
    "\n",
    "    vocab, train_dataset_pos = build_vocab(train_dataset)\n",
    "\n",
    "\n",
    "\n",
    "    def load_glove_embeddings(path):\n",
    "        glove_embeddings = {}\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype='float64')\n",
    "                glove_embeddings[word] = vector\n",
    "                \n",
    "        return glove_embeddings\n",
    "\n",
    "    glove_embeddings = load_glove_embeddings('glove.6B.50d.txt')\n",
    "    vocab_word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "    def create_embedding_matrix(word_to_index, glove_embeddings):\n",
    "        # Initialize embedding matrix with zeros\n",
    "        # 50d\n",
    "        embedding_matrix = np.zeros((len(vocab)+2, 50), dtype='float64')\n",
    "        \n",
    "        # Loop thru each word in vocab\n",
    "        for word, idx in word_to_index.items():\n",
    "            # Check if word exists in glove embeddings\n",
    "            if word in glove_embeddings:\n",
    "                # Copy glove embedding to embedding matrix\n",
    "                embedding_matrix[idx] = glove_embeddings[word]\n",
    "                # If OOV, assign None first\n",
    "                \n",
    "        return embedding_matrix\n",
    "\n",
    "    embedding_matrix = create_embedding_matrix(vocab_word_to_index, glove_embeddings)\n",
    "    #handle <unk>\n",
    "    embedding_matrix[-2]=[ 0.01513297,  0.2400952 , -0.13676383,  0.13166569, -0.28283166,\n",
    "        0.10421129,  0.39747017,  0.07944959,  0.29670785,  0.05400998,\n",
    "        0.48425894,  0.26516231, -0.48021244, -0.25129253, -0.24367068,\n",
    "       -0.24188322,  0.47579495, -0.2097357 , -0.02568224, -0.31143999,\n",
    "       -0.3196337 ,  0.44878632, -0.07379564,  0.32765833, -0.49052161,\n",
    "       -0.33455611, -0.34772199, -0.05043562, -0.0898296 ,  0.04898804,\n",
    "        0.4993778 ,  0.04359836,  0.40077601, -0.31343237,  0.24126281,\n",
    "       -0.4907152 , -0.20372591, -0.32123346, -0.39554707,  0.37386547,\n",
    "        0.44720326,  0.45492689, -0.16420979,  0.42844699,  0.15748723,\n",
    "       -0.23547929, -0.33962153,  0.04243802, -0.03647524, -0.0042893 ]\n",
    "    \n",
    "    return vocab_word_to_index,embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def prep_embedding(handle_oov=False):\n",
    "    if handle_oov:\n",
    "        with open('embedding_matrix.pkl', 'rb') as file:  \n",
    "            embedding_matrix = pickle.load(file)\n",
    "            embedding_matrix = np.concatenate((embedding_matrix, np.zeros((1, 50))), axis=0)\n",
    "        with open('vocab_word_to_index.pkl', 'rb') as file:  \n",
    "            vocab_word_to_index = pickle.load(file)\n",
    "            del vocab_word_to_index['<UNK>']\n",
    "    else:\n",
    "        vocab_word_to_index,embedding_matrix= prep_pretrained_embedding()\n",
    "        embedding_matrix[-1]=np.zeros(50)\n",
    "    # print(embedding_matrix)\n",
    "    # print(embedding_matrix.shape)\n",
    "    # print(len(vocab_word_to_index))\n",
    "    # print(vocab_word_to_index)\n",
    "    return vocab_word_to_index,embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "device=torch.device('cuda')\n",
    "\n",
    "class CustomedDataset(Dataset):\n",
    "    def __init__(self,sentences,labels,vocab_word_to_index):\n",
    "        self.features=torch.tensor([[vocab_word_to_index[word] if word in vocab_word_to_index else len(vocab_word_to_index) for word in sentence]+[len(vocab_word_to_index)+1]*(max_len-len(sentence)) for sentence in sentences]).to(device)\n",
    "        self.labels=torch.tensor(labels).to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.features[idx],self.labels[idx]\n",
    "\n",
    "def prep_dataloader(train_dataset,validation_dataset,test_dataset,batch_size,vocab_word_to_index):\n",
    "    train_dataloader=DataLoader(CustomedDataset(train_dataset[\"text\"],train_dataset[\"label\"],vocab_word_to_index),batch_size=batch_size,shuffle=True)\n",
    "    validation_dataloader=DataLoader(CustomedDataset(validation_dataset[\"text\"],validation_dataset[\"label\"],vocab_word_to_index),batch_size=batch_size)\n",
    "    test_dataloader=DataLoader(CustomedDataset(test_dataset[\"text\"],test_dataset[\"label\"],vocab_word_to_index),batch_size=batch_size)\n",
    "    return train_dataloader,validation_dataloader,test_dataloader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, num_layers = 1, freeze_embeddings=True):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "        embedding_matrix=torch.tensor(embedding_matrix,dtype=torch.float32)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=freeze_embeddings)\n",
    "        self.rnn = nn.RNN(embedding_matrix.shape[1], hidden_dim, num_layers, batch_first=True, device= device)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim, device= device)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        out, hidden = self.rnn(embedded)\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Using the last hidden state for classification\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class CNNTextClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, n_filters, filter_sizes, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        embedding_matrix=torch.tensor(embedding_matrix,dtype=torch.float32)\n",
    "        # print(embedding_matrix)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        # self.embedding = embedding_matrix\n",
    "        # print(self.embedding.shape)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv2d(1, n_filters, (fs, embedding_matrix.shape[1])) for fs in filter_sizes]\n",
    "        )\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax=nn.Softmax(-1)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        # text = [batch size, sent len]\n",
    "        # embedded=[[self.embedding[idx] for idx in sentence] for sentence in sentences]\n",
    "        embedded = self.embedding(sentences)  # embedded = [batch size, sent len, emb dim]\n",
    "        embedded = embedded.unsqueeze(1)  # embedded = [batch size, 1, sent len, emb dim]\n",
    "        # print(embedded)\n",
    "        # print(embedded.shape)\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]  # conv_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]  # pooled_n = [batch size, n_filters]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))  # cat = [batch size, n_filters * len(filter_sizes)]\n",
    "        \n",
    "        return self.softmax(self.fc(cat))\n",
    "\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_size, num_layers, output_size = 1, model_type = 'lstm'):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        vocab_size, embedding_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype = torch.float32), freeze = False, padding_idx = vocab_size-1)\n",
    "\n",
    "        if model_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers = num_layers, bidirectional = True, batch_first = True)\n",
    "        elif model_type == 'gru':\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_size, num_layers = num_layers, bidirectional = True, batch_first = True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)  # 2 for bidirectional and 1 output class\n",
    "        # self.softmax = nn.Softmax(dim = 1)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Initialize Embedding Layer\n",
    "        nn.init.uniform_(self.embedding.weight, -0.01, 0.01)\n",
    "\n",
    "        # Initialize RNN (LSTM/GRU) weights and biases\n",
    "        for name, param in self.rnn.named_parameters():\n",
    "            if 'weight_ih' in name:  # Input to hidden weights\n",
    "                nn.init.xavier_uniform_(param.data)  # Xavier initialization\n",
    "            elif 'weight_hh' in name:  # Hidden to hidden weights\n",
    "                nn.init.orthogonal_(param.data)  # Orthogonal initialization\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param.data)  # Zero bias initialization\n",
    "\n",
    "        # Initialize Linear (Fully connected) layer\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = torch.eq(x, self.embedding.num_embeddings-1)\n",
    "        lengths= mask.float().argmax(dim=1)-1\n",
    "        print(x)\n",
    "        print(lengths)\n",
    "        embedded = self.embedding(x)\n",
    "        packed_embedded = pack_padded_sequence(embedded, lengths.cpu(), batch_first = True, enforce_sorted = False)\n",
    "        packed_rnn_out, _ = self.rnn(packed_embedded)\n",
    "        rnn_out, _ = pad_packed_sequence(packed_rnn_out, batch_first = True)\n",
    "        final_feature_map = rnn_out[torch.arange(rnn_out.size(0)), lengths - 1]\n",
    "        final_out = self.fc(final_feature_map)\n",
    "        return final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc=nn.Linear(input_dim,output_dim)\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x+self.fc(x))\n",
    "\n",
    "class CNNTextResidualClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, n_filters, filter_sizes, output_dim, dropout,num_hidden=256,res_block_num=3):\n",
    "        super().__init__()\n",
    "        embedding_matrix=torch.tensor(embedding_matrix,dtype=torch.float32)\n",
    "        # print(embedding_matrix)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        # self.embedding = embedding_matrix\n",
    "        # print(self.embedding.shape)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv2d(1, n_filters, (fs, embedding_matrix.shape[1])) for fs in filter_sizes]\n",
    "        )\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, num_hidden)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.res_block=nn.Sequential(*[ResidualBlock(num_hidden,num_hidden) for _ in range(res_block_num)])\n",
    "        self.fc_out=nn.Linear(num_hidden,output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax=nn.Softmax(-1)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        # text = [batch size, sent len]\n",
    "        # embedded=[[self.embedding[idx] for idx in sentence] for sentence in sentences]\n",
    "        embedded = self.embedding(sentences)  # embedded = [batch size, sent len, emb dim]\n",
    "        embedded = embedded.unsqueeze(1)  # embedded = [batch size, 1, sent len, emb dim]\n",
    "        # print(embedded)\n",
    "        # print(embedded.shape)\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]  # conv_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]  # pooled_n = [batch size, n_filters]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))  # cat = [batch size, n_filters * len(filter_sizes)]\n",
    "        res_block_in=self.relu(self.fc(cat))\n",
    "        res_block_out=self.res_block(res_block_in)\n",
    "        return self.softmax(self.fc_out(res_block_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # 创建一个大的位置编码矩阵\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # 将位置编码作为常量注册到模型中，不需要梯度更新\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        # print(self.pe.shape)\n",
    "        x = x + self.pe[:,:x.shape[1], :].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "def get_key_padding_mask(tokens,vocab_size):\n",
    "    key_padding_mask=torch.zeros(tokens.size())\n",
    "    key_padding_mask[tokens==vocab_size-1]=-torch.inf\n",
    "    return key_padding_mask.to(device)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, embedding_matrix, max_len, dropout, num_hidden=64, num_resblock=2, nhead=3, num_encoder_layers=1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.nhead=nhead\n",
    "        embedding_matrix=torch.tensor(embedding_matrix,dtype=torch.float32)\n",
    "        d_model=embedding_matrix.shape[1]\n",
    "        self.embedding_src = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.positional_encoding = PositionalEncoding(d_model,max_len=max_len)  # 最大序列长度为1650\n",
    "        encoder = nn.TransformerEncoderLayer(d_model=d_model*nhead, nhead=nhead,dim_feedforward=64)\n",
    "        self.transformer = nn.TransformerEncoder(encoder,num_encoder_layers)\n",
    "        self.fc1 = nn.Linear(d_model*nhead, num_hidden)\n",
    "        self.res_blocks1=nn.Sequential(*[ResidualBlock(num_hidden,num_hidden) for _ in range(num_resblock)])\n",
    "        self.fc2 = nn.Linear(num_hidden,num_hidden)\n",
    "        # self.fc2 = nn.Linear(num_hidden*max_len,num_hidden)\n",
    "        self.res_blocks2=nn.Sequential(*[ResidualBlock(num_hidden,num_hidden) for _ in range(num_resblock)])\n",
    "        self.fc3 = nn.Linear(num_hidden,2)\n",
    "        self.softmax=nn.Softmax(-1)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src_key_padding_mask = get_key_padding_mask(src,self.embedding_src.num_embeddings)\n",
    "        src_emb = self.positional_encoding(self.embedding_src(src))\n",
    "        # Transformer forward with attention masks\n",
    "        output = self.transformer(\n",
    "            src_emb.unsqueeze(2).repeat(1,1,self.nhead,1).reshape(src_emb.shape[0],src_emb.shape[1],-1).permute(1, 0, 2), \n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "        ) #[token_num,batch_size,embedding_dim]\n",
    "        # pooled_output=nn.MaxPool2d(,)\n",
    "        output=self.dropout(self.relu(self.fc1(output.permute(1,0,2))))\n",
    "        output=self.res_blocks1(output)\n",
    "        output=self.dropout(self.relu(self.fc2(torch.max(output,dim=1)[0])))\n",
    "        # output=self.dropout(self.relu(self.fc2(output.reshape(output.shape[0],-1))))\n",
    "        output=self.res_blocks2(output)\n",
    "        return self.softmax(self.fc3(output))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model,optimizer,criterion,num_epoch,train_dataloader,validation_dataloader):\n",
    "    from tqdm import tqdm\n",
    "    model.to(device)\n",
    "    for _ in range(num_epoch):\n",
    "        acc_loss=0\n",
    "        model.train()\n",
    "        process_bar=tqdm(train_dataloader,desc=f\"Epoch {_}/{num_epoch}\",leave=True)\n",
    "        for features,labels in process_bar:\n",
    "            \n",
    "            pred=model(features)\n",
    "            # print(pred,pred.shape)\n",
    "            # print(labels,labels.shape)\n",
    "            optimizer.zero_grad()\n",
    "            loss=criterion(pred,labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            acc_loss+=loss.item()\n",
    "            process_bar.set_postfix_str(f\"Mean loss: {acc_loss/(process_bar.n+1)}\")\n",
    "        \n",
    "        print(\"Train loss:\",acc_loss/process_bar.n)\n",
    "        \n",
    "        acc_loss=0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            acc_loss=0\n",
    "            process_bar=tqdm(validation_dataloader,desc=\"Validating\",leave=True)\n",
    "            for features,labels in process_bar:\n",
    "                \n",
    "                pred=model(features)\n",
    "                \n",
    "                loss=criterion(pred,labels)\n",
    "                \n",
    "                acc_loss+=loss.item()\n",
    "                process_bar.set_postfix_str(f\"Mean loss: {acc_loss/(process_bar.n+1)}\")\n",
    "                \n",
    "            print(\"Validation loss:\",acc_loss/process_bar.n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/10:   0%|          | 0/267 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   87,  9231, 12022,  ..., 17842, 17842, 17842],\n",
      "        [ 2492, 11722,  5722,  ..., 17842, 17842, 17842],\n",
      "        [  882, 12022,  3850,  ..., 17842, 17842, 17842],\n",
      "        ...,\n",
      "        [ 5722, 11722, 17841,  ..., 17842, 17842, 17842],\n",
      "        [12022, 17841,  1198,  ..., 17842, 17842, 17842],\n",
      "        [12022,    87, 17841,  ..., 17842, 17842, 17842]], device='cuda:0')\n",
      "tensor([111, 114, 163, 122,  89,  83, 248, 130, 121, 143, 134,  42, 131, 130,\n",
      "         10,  64,  91,  40, 122, 161,  66, 198,  47, 184, 210,  77, 168,  96,\n",
      "         59,  95,  75,  98], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/10:   0%|          | 0/267 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest acc is:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;241m/\u001b[39mtot_samples\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m params\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m32\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_filters\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m32\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilter_sizes\u001b[39m\u001b[38;5;124m\"\u001b[39m:[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m5\u001b[39m],\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m128\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m2\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m0.01\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_size\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m64\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m3\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m10\u001b[39m}\n\u001b[1;32m---> 34\u001b[0m \u001b[43mwork_flow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSentimentModel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 20\u001b[0m, in \u001b[0;36mwork_flow\u001b[1;34m(model_type, handle_oov, params)\u001b[0m\n\u001b[0;32m     17\u001b[0m criterion\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 20\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_epoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     23\u001b[0m test_acc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[9], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, criterion, num_epoch, train_dataloader, validation_dataloader)\u001b[0m\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     14\u001b[0m loss\u001b[38;5;241m=\u001b[39mcriterion(pred,labels)\n\u001b[1;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     18\u001b[0m acc_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32me:\\work\\miniconda\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\work\\miniconda\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "def work_flow(model_type,handle_oov,params):\n",
    "    vocab_word_to_index,embedding_matrix=prep_embedding(handle_oov)\n",
    "    # print(embedding_matrix)\n",
    "    train_dataloader,validation_dataloader,test_dataloader=prep_dataloader(train_dataset,validation_dataset,test_dataset,params[\"batch_size\"],vocab_word_to_index)\n",
    "\n",
    "    if model_type==\"CNN\":\n",
    "        model = CNNTextClassifier(embedding_matrix, params[\"n_filters\"], params[\"filter_sizes\"], params[\"output_dim\"], params[\"dropout\"])\n",
    "    if model_type==\"CNN_res_block\":\n",
    "        model = CNNTextResidualClassifier(embedding_matrix, params[\"n_filters\"], params[\"filter_sizes\"], params[\"output_dim\"], params[\"dropout\"])\n",
    "    if model_type==\"transformer\":\n",
    "        model = TransformerModel(embedding_matrix,max_len,params[\"dropout\"])\n",
    "    if model_type==\"SentimentRNN\":\n",
    "        model = SentimentRNN(embedding_matrix,params[\"hidden_dim\"],params[\"output_dim\"])\n",
    "    if model_type==\"SentimentModel\":\n",
    "        model = SentimentModel(embedding_matrix,params[\"hidden_size\"],params[\"num_layers\"])\n",
    "\n",
    "    criterion=nn.CrossEntropyLoss()\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=params[\"lr\"])\n",
    "    \n",
    "    train(model,optimizer,criterion,params[\"num_epoch\"],train_dataloader,validation_dataloader)\n",
    "    \n",
    "    model.eval()\n",
    "    test_acc=0\n",
    "    tot_samples=0\n",
    "    with torch.no_grad():\n",
    "        for features,labels in test_dataloader:\n",
    "            pred_labels=model(features)\n",
    "            # print(labels.shape,pred_labels.shape)\n",
    "            test_acc+=(labels==pred_labels.argmax(dim=1)).sum().item()\n",
    "            tot_samples+=labels.shape[0]\n",
    "        print(f\"Test acc is:{test_acc/tot_samples*100}%\")\n",
    "\n",
    "params={\"batch_size\":32,\"n_filters\":32,\"filter_sizes\":[1,2,3,5],\"hidden_dim\":128,\"output_dim\":2,\"dropout\":0.1,\"lr\":0.01,\"hidden_size\":64,\"num_layers\":3,\"num_epoch\":10}\n",
    "work_flow(\"SentimentModel\",True,params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
