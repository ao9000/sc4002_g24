{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267\n"
     ]
    }
   ],
   "source": [
    "# Load the Rotten Tomatoes dataset\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train']\n",
    "valid_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "train_text = train_dataset.to_pandas()['text']\n",
    "\n",
    "max_text_len = 0\n",
    "for text in train_text:\n",
    "    max_text_len = max(max_text_len, len(text))\n",
    "\n",
    "print(max_text_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Load the embedding matrix and vocab from files\n",
    "with open('embedding_matrix.pkl', 'rb') as f:\n",
    "    embedding_matrix = pickle.load(f).astype(np.float32)\n",
    "    padding = [0 for i in range(50)]\n",
    "    embedding_matrix = np.insert(embedding_matrix, 0, padding, 0)\n",
    "    print(type(embedding_matrix))\n",
    "\n",
    "with open('vocab_word_to_index.pkl', 'rb') as f:\n",
    "    vocab_word_to_index = pickle.load(f)\n",
    "    print(type(vocab_word_to_index))\n",
    "\n",
    "# Convert to torch tensors\n",
    "embedding_matrix = torch.tensor(embedding_matrix)\n",
    "vocab_size, embedding_dim = embedding_matrix.shape\n",
    "print(embedding_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/adityakumarpugalia/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, dataset, word_to_index, max_len=max_text_len):\n",
    "        self.dataset = dataset\n",
    "        self.word_to_index = word_to_index\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[idx]['text']\n",
    "        label = self.dataset[idx]['label']\n",
    "        \n",
    "        # Convert words to indices\n",
    "        \n",
    "        text = text.lower()\n",
    "        #tokenise words\n",
    "        word_list = nltk.tokenize.word_tokenize(text)\n",
    "        word_list = [word.strip(\"'\\\"\") for word in word_list]\n",
    "        tokens = set()\n",
    "        tokens.update(word_list)\n",
    "        #tokens.discard('')\n",
    "        indices = [self.word_to_index.get(word, self.word_to_index.get('<UNK>')) + 1 for word in tokens]\n",
    "        indices = indices[:self.max_len] + [0] * (self.max_len - len(indices))  # Padding\n",
    "        \n",
    "        return torch.tensor(indices), torch.tensor(label)\n",
    "\n",
    "train_data = SentimentDataset(train_dataset, vocab_word_to_index)\n",
    "valid_data = SentimentDataset(valid_dataset, vocab_word_to_index)\n",
    "test_data = SentimentDataset(test_dataset, vocab_word_to_index)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4901, -0.2097,  0.1249, -0.3952, -0.4148,  0.7488,  0.3754,  0.3650,\n",
      "        -0.0177, -0.3884,  0.1328, -0.3996, -0.6031,  0.6804, -0.5267,  1.0355,\n",
      "         0.8665,  0.2221,  0.6303, -0.8495,  0.6170, -0.0247,  0.8059, -0.1039,\n",
      "         0.1430,  0.2291, -0.7631,  1.6906,  1.1369, -0.7731,  0.7997, -0.0726,\n",
      "         1.0869,  0.1207, -0.0339,  0.8330, -0.3656,  0.5224,  0.5808,  0.4711,\n",
      "        -0.2782, -0.5437,  0.3406,  0.4131, -0.2171, -0.4031,  0.5080, -0.0770,\n",
      "        -0.2399, -1.4470])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, num_layers = 1, freeze_embeddings=True):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=freeze_embeddings)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True, device= device)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim, device= device)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        out, hidden = self.rnn(embedded)\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Using the last hidden state for classification\n",
    "        return self.fc(out)\n",
    "\n",
    "# Model hyperparameters\n",
    "hidden_dim = 128\n",
    "output_dim = 2  # Binary classification (positive, negative)\n",
    "\n",
    "model = SentimentRNN(embedding_matrix, hidden_dim, output_dim, 2)\n",
    "print(model.embedding.weight[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Move model to GPU if available\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, valid_loader, epochs=10):\n",
    "    best_valid_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "        \n",
    "        for texts, labels in train_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            \n",
    "            # Forward pass: get predictions\n",
    "            predictions = model(texts)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(predictions, labels)\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # Backpropagation and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accuracy calculation\n",
    "            predicted_labels = predictions.argmax(1)\n",
    "            total_train_correct += (predicted_labels == labels).sum().item()\n",
    "            total_train_samples += labels.size(0)\n",
    "\n",
    "        # Calculate and print average training accuracy and loss per epoch\n",
    "        train_accuracy = total_train_correct / total_train_samples\n",
    "        train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # Evaluate model on validation set\n",
    "        valid_accuracy = evaluate_model(model, valid_loader)\n",
    "        \n",
    "        print(f'Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f} | Validation Accuracy: {valid_accuracy:.4f}')        \n",
    "        if valid_accuracy > best_valid_acc:\n",
    "            best_valid_acc = valid_accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            predictions = model(texts)\n",
    "            correct += (predictions.argmax(1) == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.6941 | Train Accuracy: 0.4998 | Validation Accuracy: 0.5000\n",
      "Epoch 2 | Train Loss: 0.6936 | Train Accuracy: 0.4897 | Validation Accuracy: 0.5000\n",
      "Epoch 3 | Train Loss: 0.6933 | Train Accuracy: 0.5054 | Validation Accuracy: 0.5000\n",
      "Epoch 4 | Train Loss: 0.6933 | Train Accuracy: 0.5012 | Validation Accuracy: 0.5000\n",
      "Epoch 5 | Train Loss: 0.6934 | Train Accuracy: 0.4925 | Validation Accuracy: 0.5000\n",
      "Epoch 6 | Train Loss: 0.6932 | Train Accuracy: 0.5009 | Validation Accuracy: 0.5000\n",
      "Epoch 7 | Train Loss: 0.6933 | Train Accuracy: 0.5021 | Validation Accuracy: 0.5000\n",
      "Epoch 8 | Train Loss: 0.6935 | Train Accuracy: 0.4951 | Validation Accuracy: 0.5000\n",
      "Epoch 9 | Train Loss: 0.6933 | Train Accuracy: 0.4955 | Validation Accuracy: 0.5000\n",
      "Epoch 10 | Train Loss: 0.6933 | Train Accuracy: 0.4909 | Validation Accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model, train_loader, valid_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_acc = evaluate_model(model, test_loader)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
