{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train'].to_pandas()\n",
    "validation_dataset = dataset['validation'].to_pandas()\n",
    "test_dataset = dataset['test'].to_pandas()\n",
    "max_len=max(0,train_dataset[\"text\"].apply(lambda x:len(x)).max())\n",
    "max_len=max(max_len,validation_dataset[\"text\"].apply(lambda x:len(x)).max())\n",
    "max_len=max(max_len,test_dataset[\"text\"].apply(lambda x:len(x)).max())\n",
    "max_len+=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def prep_pretrained_embedding():\n",
    "    def build_vocab(train_dataset):\n",
    "        # Create set, unique words only\n",
    "        vocab = set()\n",
    "        train_dataset_pos = []\n",
    "        \n",
    "        # Loop thru each sentence in training dataset\n",
    "        for sentence in train_dataset['text']:\n",
    "            # Basic text processing\n",
    "            \n",
    "            # Case folding\n",
    "            sentence = sentence.lower()\n",
    "            \n",
    "            # NLTK tokenizer does a good job at separating meaningful words + punctuations\n",
    "            # Better than defining regex ourselves\n",
    "            word_list = nltk.tokenize.word_tokenize(sentence)\n",
    "            \n",
    "            # # Further split words into separate words\n",
    "            # # e.g., 'well-being' -> 'well', 'being'\n",
    "            # # e.g., 'music/song' -> 'music', 'song'\n",
    "            # split_word_list = []\n",
    "            # for word in sentence_list:\n",
    "            #     split_word_list.extend(word.replace('-', ' ').replace('/', ' ').split())\n",
    "            \n",
    "            # Dont remove all special characters, some are meaningful\n",
    "            # Some words are surrounded by single/double quotes\n",
    "            word_list = [word.strip(\"'\\\"\") for word in word_list]\n",
    "            \n",
    "            # Add into set\n",
    "            vocab.update(word_list)\n",
    "            \n",
    "            # Get pos tags\n",
    "            # Also build POS tags\n",
    "            pos_tags = nltk.pos_tag(word_list)\n",
    "            train_dataset_pos.append(pos_tags)\n",
    "            \n",
    "        vocab.discard('')\n",
    "        return vocab, train_dataset_pos\n",
    "\n",
    "    vocab, train_dataset_pos = build_vocab(train_dataset)\n",
    "\n",
    "\n",
    "\n",
    "    def load_glove_embeddings(path):\n",
    "        glove_embeddings = {}\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype='float64')\n",
    "                glove_embeddings[word] = vector\n",
    "                \n",
    "        return glove_embeddings\n",
    "\n",
    "    glove_embeddings = load_glove_embeddings('glove.6B.50d.txt')\n",
    "    vocab_word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "    def create_embedding_matrix(word_to_index, glove_embeddings):\n",
    "        # Initialize embedding matrix with zeros\n",
    "        # 50d\n",
    "        embedding_matrix = np.zeros((len(vocab)+2, 50), dtype='float64')\n",
    "        \n",
    "        # Loop thru each word in vocab\n",
    "        for word, idx in word_to_index.items():\n",
    "            # Check if word exists in glove embeddings\n",
    "            if word in glove_embeddings:\n",
    "                # Copy glove embedding to embedding matrix\n",
    "                embedding_matrix[idx] = glove_embeddings[word]\n",
    "                # If OOV, assign None first\n",
    "                \n",
    "        return embedding_matrix\n",
    "\n",
    "    embedding_matrix = create_embedding_matrix(vocab_word_to_index, glove_embeddings)\n",
    "    #handle <unk>\n",
    "    embedding_matrix[-2]=[ 0.01513297,  0.2400952 , -0.13676383,  0.13166569, -0.28283166,\n",
    "        0.10421129,  0.39747017,  0.07944959,  0.29670785,  0.05400998,\n",
    "        0.48425894,  0.26516231, -0.48021244, -0.25129253, -0.24367068,\n",
    "       -0.24188322,  0.47579495, -0.2097357 , -0.02568224, -0.31143999,\n",
    "       -0.3196337 ,  0.44878632, -0.07379564,  0.32765833, -0.49052161,\n",
    "       -0.33455611, -0.34772199, -0.05043562, -0.0898296 ,  0.04898804,\n",
    "        0.4993778 ,  0.04359836,  0.40077601, -0.31343237,  0.24126281,\n",
    "       -0.4907152 , -0.20372591, -0.32123346, -0.39554707,  0.37386547,\n",
    "        0.44720326,  0.45492689, -0.16420979,  0.42844699,  0.15748723,\n",
    "       -0.23547929, -0.33962153,  0.04243802, -0.03647524, -0.0042893 ]\n",
    "    \n",
    "    return vocab_word_to_index,embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def prep_embedding(handle_oov=False):\n",
    "    if handle_oov:\n",
    "        with open('embedding_matrix.pkl', 'rb') as file:  \n",
    "            embedding_matrix = pickle.load(file)\n",
    "            embedding_matrix = np.concatenate((embedding_matrix, np.zeros((1, 50))), axis=0)\n",
    "        with open('vocab_word_to_index.pkl', 'rb') as file:  \n",
    "            vocab_word_to_index = pickle.load(file)\n",
    "            del vocab_word_to_index['<UNK>']\n",
    "    else:\n",
    "        vocab_word_to_index,embedding_matrix= prep_pretrained_embedding()\n",
    "        embedding_matrix[-1]=np.zeros(50)\n",
    "    # print(embedding_matrix)\n",
    "    # print(embedding_matrix.shape)\n",
    "    # print(len(vocab_word_to_index))\n",
    "    # print(vocab_word_to_index)\n",
    "    return vocab_word_to_index,embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "device=torch.device('cuda')\n",
    "\n",
    "class CustomedDataset(Dataset):\n",
    "    def __init__(self,sentences,labels,vocab_word_to_index):\n",
    "        self.features=torch.tensor([[vocab_word_to_index[word] if word in vocab_word_to_index else len(vocab_word_to_index) for word in sentence]+[len(vocab_word_to_index)+1]*(max_len-len(sentence)) for sentence in sentences]).to(device)\n",
    "        self.labels=torch.tensor(labels).to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.features[idx],self.labels[idx]\n",
    "\n",
    "def prep_dataloader(train_dataset,validation_dataset,test_dataset,batch_size,vocab_word_to_index):\n",
    "    train_dataloader=DataLoader(CustomedDataset(train_dataset[\"text\"],train_dataset[\"label\"],vocab_word_to_index),batch_size=batch_size,shuffle=True)\n",
    "    validation_dataloader=DataLoader(CustomedDataset(validation_dataset[\"text\"],validation_dataset[\"label\"],vocab_word_to_index),batch_size=batch_size)\n",
    "    test_dataloader=DataLoader(CustomedDataset(test_dataset[\"text\"],test_dataset[\"label\"],vocab_word_to_index),batch_size=batch_size)\n",
    "    return train_dataloader,validation_dataloader,test_dataloader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CNNTextClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, n_filters, filter_sizes, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        embedding_matrix=torch.tensor(embedding_matrix,dtype=torch.float32)\n",
    "        # print(embedding_matrix)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        # self.embedding = embedding_matrix\n",
    "        # print(self.embedding.shape)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv2d(1, n_filters, (fs, embedding_matrix.shape[1])) for fs in filter_sizes]\n",
    "        )\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax=nn.Softmax(-1)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        # text = [batch size, sent len]\n",
    "        # embedded=[[self.embedding[idx] for idx in sentence] for sentence in sentences]\n",
    "        embedded = self.embedding(sentences)  # embedded = [batch size, sent len, emb dim]\n",
    "        embedded = embedded.unsqueeze(1)  # embedded = [batch size, 1, sent len, emb dim]\n",
    "        # print(embedded)\n",
    "        # print(embedded.shape)\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]  # conv_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]  # pooled_n = [batch size, n_filters]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))  # cat = [batch size, n_filters * len(filter_sizes)]\n",
    "        \n",
    "        return self.softmax(self.fc(cat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model,optimizer,criterion,num_epoch,train_dataloader,validation_dataloader):\n",
    "    from tqdm import tqdm\n",
    "    model.to(device)\n",
    "    for _ in range(num_epoch):\n",
    "        acc_loss=0\n",
    "        model.train()\n",
    "        process_bar=tqdm(train_dataloader,desc=f\"Epoch {_}/{num_epoch}\",leave=True)\n",
    "        for features,labels in process_bar:\n",
    "            \n",
    "            pred=model(features)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss=criterion(pred,labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            acc_loss+=loss.item()\n",
    "            process_bar.set_postfix_str(f\"Mean loss: {acc_loss/(process_bar.n+1)}\")\n",
    "        \n",
    "        print(\"Train loss:\",acc_loss/process_bar.n)\n",
    "        \n",
    "        acc_loss=0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            acc_loss=0\n",
    "            process_bar=tqdm(validation_dataloader,desc=\"Validating\",leave=True)\n",
    "            for features,labels in process_bar:\n",
    "                \n",
    "                pred=model(features)\n",
    "                \n",
    "                loss=criterion(pred,labels)\n",
    "                \n",
    "                acc_loss+=loss.item()\n",
    "                process_bar.set_postfix_str(f\"Mean loss: {acc_loss/(process_bar.n+1)}\")\n",
    "                \n",
    "            print(\"Validation loss:\",acc_loss/process_bar.n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/30: 100%|██████████| 34/34 [00:01<00:00, 24.56it/s, Mean loss: 0.694391527596642] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.694391527596642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 84.03it/s, Mean loss: 3.3717047572135925]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6743409514427186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|██████████| 34/34 [00:00<00:00, 39.48it/s, Mean loss: 0.6960725964921893]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6755998730659485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 88.17it/s, Mean loss: 3.402778387069702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6805556774139404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|██████████| 34/34 [00:00<00:00, 39.98it/s, Mean loss: 0.681260220932238] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6612231556107017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 89.98it/s, Mean loss: 3.3044593334198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6608918666839599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|██████████| 34/34 [00:00<00:00, 39.78it/s, Mean loss: 0.7122497097138436]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6494041470920339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 85.53it/s, Mean loss: 3.3273062705993652]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.665461254119873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 100%|██████████| 34/34 [00:00<00:00, 40.03it/s, Mean loss: 0.695120140429466] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6337860103915719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 78.09it/s, Mean loss: 3.1803221702575684]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6360644340515137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: 100%|██████████| 34/34 [00:00<00:00, 40.03it/s, Mean loss: 0.6458510900988723]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6268554698018467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 90.87it/s, Mean loss: 3.123402953147888]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6246805906295776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30: 100%|██████████| 34/34 [00:00<00:00, 39.61it/s, Mean loss: 0.6093016775215373]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6093016775215373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 87.43it/s, Mean loss: 3.1291255354881287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6258251070976257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30: 100%|██████████| 34/34 [00:00<00:00, 39.82it/s, Mean loss: 0.6641861450287604]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6055814851732815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 88.08it/s, Mean loss: 3.1332091093063354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.626641821861267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30: 100%|██████████| 34/34 [00:00<00:00, 39.64it/s, Mean loss: 0.6084854602813721]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5905888290966258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 87.98it/s, Mean loss: 3.162788450717926]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6325576901435852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30: 100%|██████████| 34/34 [00:00<00:00, 39.70it/s, Mean loss: 0.6353477874109822]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5792876885217779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 80.07it/s, Mean loss: 3.0534550547599792]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6106910109519958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30: 100%|██████████| 34/34 [00:00<00:00, 39.11it/s, Mean loss: 0.5863286870898623]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5690837257048663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 79.59it/s, Mean loss: 3.1180153489112854]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6236030697822571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30: 100%|██████████| 34/34 [00:00<00:00, 39.90it/s, Mean loss: 0.5693994497551638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5693994497551638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 82.85it/s, Mean loss: 3.0428932905197144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6085786581039428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30: 100%|██████████| 34/34 [00:00<00:00, 39.34it/s, Mean loss: 0.5744770945924701]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5575807094573975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 82.93it/s, Mean loss: 3.3500396609306335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6700079321861268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30: 100%|██████████| 34/34 [00:00<00:00, 39.81it/s, Mean loss: 0.5510136765592238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5510136765592238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 86.30it/s, Mean loss: 3.010809063911438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6021618127822876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30: 100%|██████████| 34/34 [00:00<00:00, 39.44it/s, Mean loss: 0.5612619121869405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5447542088873246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 89.46it/s, Mean loss: 2.965977132320404]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5931954264640809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30: 100%|██████████| 34/34 [00:00<00:00, 39.68it/s, Mean loss: 0.5564098683270541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5400448721997878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 87.61it/s, Mean loss: 2.9776124358177185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5955224871635437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30: 100%|██████████| 34/34 [00:00<00:00, 38.88it/s, Mean loss: 0.5466898878415426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5306107734932619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 81.38it/s, Mean loss: 3.069404900074005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6138809800148011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30: 100%|██████████| 34/34 [00:00<00:00, 38.80it/s, Mean loss: 0.5441874672066082]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5281819534652373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 82.08it/s, Mean loss: 2.9362486600875854]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5872497320175171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30: 100%|██████████| 34/34 [00:00<00:00, 38.29it/s, Mean loss: 0.5432988567785784]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5273194786380319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 82.18it/s, Mean loss: 2.9878429770469666]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5975685954093933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30: 100%|██████████| 34/34 [00:00<00:00, 39.87it/s, Mean loss: 0.5142481563722386]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5142481563722386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 88.04it/s, Mean loss: 2.9849127531051636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5969825506210327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30: 100%|██████████| 34/34 [00:00<00:00, 39.84it/s, Mean loss: 0.5093022006399491]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5093022006399491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 83.01it/s, Mean loss: 2.9300588965415955]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.586011779308319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30: 100%|██████████| 34/34 [00:00<00:00, 39.22it/s, Mean loss: 0.5208939620942781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5055735514444464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 83.37it/s, Mean loss: 2.89328470826149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.578656941652298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30: 100%|██████████| 34/34 [00:00<00:00, 39.45it/s, Mean loss: 0.509218825136914] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.509218825136914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 87.33it/s, Mean loss: 3.0786243081092834]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6157248616218567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30: 100%|██████████| 34/34 [00:00<00:00, 39.57it/s, Mean loss: 0.49543416237129884]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.49543416237129884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 84.03it/s, Mean loss: 3.0199597477912903]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6039919495582581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30: 100%|██████████| 34/34 [00:00<00:00, 39.83it/s, Mean loss: 0.49751485884189606]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.49751485884189606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 83.27it/s, Mean loss: 2.8901124596595764]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5780224919319152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30: 100%|██████████| 34/34 [00:00<00:00, 39.44it/s, Mean loss: 0.49627482891082764]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.49627482891082764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 87.85it/s, Mean loss: 2.880316436290741]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5760632872581481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30: 100%|██████████| 34/34 [00:00<00:00, 39.41it/s, Mean loss: 0.5014495967012463]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.48670107915120964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 77.28it/s, Mean loss: 2.9862104654312134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5972420930862427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30: 100%|██████████| 34/34 [00:00<00:00, 39.53it/s, Mean loss: 0.5276506937319233] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4810932795791065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 87.64it/s, Mean loss: 2.947641611099243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5895283222198486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30: 100%|██████████| 34/34 [00:00<00:00, 34.84it/s, Mean loss: 0.4939006232854092] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.47937413436525006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 89.19it/s, Mean loss: 3.1967454850673676]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6393490970134735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30: 100%|██████████| 34/34 [00:00<00:00, 39.56it/s, Mean loss: 0.49930560408216534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.48462014513857227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 83.23it/s, Mean loss: 2.9429466128349304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.588589322566986\n",
      "Test acc is:73.26454033771107%\n"
     ]
    }
   ],
   "source": [
    "def work_flow(model_type,handle_oov,params):\n",
    "    vocab_word_to_index,embedding_matrix=prep_embedding(handle_oov)\n",
    "    # print(embedding_matrix)\n",
    "    train_dataloader,validation_dataloader,test_dataloader=prep_dataloader(train_dataset,validation_dataset,test_dataset,params[\"batch_size\"],vocab_word_to_index)\n",
    "\n",
    "    if model_type==\"CNN\":\n",
    "        model = CNNTextClassifier(embedding_matrix, params[\"n_filters\"], params[\"filter_sizes\"], params[\"output_dim\"], params[\"dropout\"])\n",
    "\n",
    "    criterion=nn.CrossEntropyLoss()\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=params[\"lr\"])\n",
    "    \n",
    "    train(model,optimizer,criterion,params[\"num_epoch\"],train_dataloader,validation_dataloader)\n",
    "    \n",
    "    model.eval()\n",
    "    test_acc=0\n",
    "    tot_samples=0\n",
    "    with torch.no_grad():\n",
    "        for features,labels in test_dataloader:\n",
    "            pred_labels=model(features)\n",
    "            # print(labels.shape,pred_labels.shape)\n",
    "            test_acc+=(labels==pred_labels.argmax(dim=1)).sum().item()\n",
    "            tot_samples+=labels.shape[0]\n",
    "        print(f\"Test acc is:{test_acc/tot_samples*100}%\")\n",
    "\n",
    "params={\"batch_size\":256,\"n_filters\":128,\"filter_sizes\":[1,2,3,5],\"output_dim\":2,\"dropout\":0.2,\"lr\":0.001,\"num_epoch\":30}\n",
    "work_flow(\"CNN\",True,params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
