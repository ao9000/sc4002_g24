{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train'].to_pandas()\n",
    "validation_dataset = dataset['validation'].to_pandas()\n",
    "test_dataset = dataset['test'].to_pandas()\n",
    "max_len=max(0,train_dataset[\"text\"].apply(lambda x:len(x)).max())\n",
    "max_len=max(max_len,validation_dataset[\"text\"].apply(lambda x:len(x)).max())\n",
    "max_len=max(max_len,test_dataset[\"text\"].apply(lambda x:len(x)).max())\n",
    "max_len+=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def prep_pretrained_embedding():\n",
    "    #copy from part 1\n",
    "    def build_vocab(train_dataset):\n",
    "        # Create set, unique words only\n",
    "        vocab = set()\n",
    "        train_dataset_pos = []\n",
    "        \n",
    "        # Loop thru each sentence in training dataset\n",
    "        for sentence in train_dataset['text']:\n",
    "            # Basic text processing\n",
    "            \n",
    "            # Case folding\n",
    "            sentence = sentence.lower()\n",
    "            \n",
    "            # NLTK tokenizer does a good job at separating meaningful words + punctuations\n",
    "            # Better than defining regex ourselves\n",
    "            word_list = nltk.tokenize.word_tokenize(sentence)\n",
    "            \n",
    "            # # Further split words into separate words\n",
    "            # # e.g., 'well-being' -> 'well', 'being'\n",
    "            # # e.g., 'music/song' -> 'music', 'song'\n",
    "            # split_word_list = []\n",
    "            # for word in sentence_list:\n",
    "            #     split_word_list.extend(word.replace('-', ' ').replace('/', ' ').split())\n",
    "            \n",
    "            # Dont remove all special characters, some are meaningful\n",
    "            # Some words are surrounded by single/double quotes\n",
    "            word_list = [word.strip(\"'\\\"\") for word in word_list]\n",
    "            \n",
    "            # Add into set\n",
    "            vocab.update(word_list)\n",
    "            \n",
    "            # Get pos tags\n",
    "            # Also build POS tags\n",
    "            pos_tags = nltk.pos_tag(word_list)\n",
    "            train_dataset_pos.append(pos_tags)\n",
    "            \n",
    "        vocab.discard('')\n",
    "        return vocab, train_dataset_pos\n",
    "\n",
    "    vocab, train_dataset_pos = build_vocab(train_dataset)\n",
    "\n",
    "\n",
    "\n",
    "    def load_glove_embeddings(path):\n",
    "        glove_embeddings = {}\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype='float64')\n",
    "                glove_embeddings[word] = vector\n",
    "                \n",
    "        return glove_embeddings\n",
    "\n",
    "    glove_embeddings = load_glove_embeddings('glove.6B.50d.txt')\n",
    "    vocab_word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "    def create_embedding_matrix(word_to_index, glove_embeddings):\n",
    "        # Initialize embedding matrix with zeros\n",
    "        # 50d\n",
    "        embedding_matrix = np.zeros((len(vocab)+2, 50), dtype='float64')\n",
    "        \n",
    "        # Loop thru each word in vocab\n",
    "        for word, idx in word_to_index.items():\n",
    "            # Check if word exists in glove embeddings\n",
    "            if word in glove_embeddings:\n",
    "                # Copy glove embedding to embedding matrix\n",
    "                embedding_matrix[idx] = glove_embeddings[word]\n",
    "                # If OOV, assign None first\n",
    "                \n",
    "        return embedding_matrix\n",
    "\n",
    "    embedding_matrix = create_embedding_matrix(vocab_word_to_index, glove_embeddings)\n",
    "    #handle <unk>\n",
    "    embedding_matrix[-2]=[ 0.01513297,  0.2400952 , -0.13676383,  0.13166569, -0.28283166,\n",
    "        0.10421129,  0.39747017,  0.07944959,  0.29670785,  0.05400998,\n",
    "        0.48425894,  0.26516231, -0.48021244, -0.25129253, -0.24367068,\n",
    "       -0.24188322,  0.47579495, -0.2097357 , -0.02568224, -0.31143999,\n",
    "       -0.3196337 ,  0.44878632, -0.07379564,  0.32765833, -0.49052161,\n",
    "       -0.33455611, -0.34772199, -0.05043562, -0.0898296 ,  0.04898804,\n",
    "        0.4993778 ,  0.04359836,  0.40077601, -0.31343237,  0.24126281,\n",
    "       -0.4907152 , -0.20372591, -0.32123346, -0.39554707,  0.37386547,\n",
    "        0.44720326,  0.45492689, -0.16420979,  0.42844699,  0.15748723,\n",
    "       -0.23547929, -0.33962153,  0.04243802, -0.03647524, -0.0042893 ]\n",
    "    \n",
    "    return vocab_word_to_index,embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def prep_embedding(handle_oov=False,model_type=\"\"):\n",
    "    \"\"\"\n",
    "    handle_oov: bool, True if implement oov handling solution discussed in part 1\n",
    "    model_type: special case for \"SentimentRNN\" in part 2\n",
    "    \"\"\"\n",
    "    if handle_oov:\n",
    "        #load from file\n",
    "        with open('embedding_matrix.pkl', 'rb') as file:  \n",
    "            embedding_matrix = pickle.load(file)\n",
    "            if model_type==\"SentimentRNN\":\n",
    "                #copy from part 2\n",
    "                padding = [0 for i in range(50)]\n",
    "                embedding_matrix = np.insert(embedding_matrix, 0, padding, 0)\n",
    "            else:\n",
    "                #add padding element\n",
    "                embedding_matrix = np.concatenate((embedding_matrix, np.zeros((1, 50))), axis=0)\n",
    "        with open('vocab_word_to_index.pkl', 'rb') as file:  \n",
    "            vocab_word_to_index = pickle.load(file)\n",
    "            if model_type!=\"SentimentRNN\":\n",
    "                #delete <UNK>\n",
    "                del vocab_word_to_index['<UNK>']\n",
    "    else:\n",
    "        vocab_word_to_index,embedding_matrix= prep_pretrained_embedding()\n",
    "        if model_type==\"SentimentRNN\":\n",
    "            #copy from part 2\n",
    "            vocab_word_to_index['<UNK>']=len(vocab_word_to_index)\n",
    "            padding = [0 for i in range(50)]\n",
    "            embedding_matrix = np.insert(embedding_matrix, 0, padding, 0)\n",
    "        else:\n",
    "            #add padding\n",
    "            embedding_matrix[-1]=np.zeros(50)\n",
    "    return vocab_word_to_index,embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset,TensorDataset\n",
    "\n",
    "#set the device\n",
    "device=torch.device('cuda')\n",
    "\n",
    "#copy from part 2\n",
    "class SentimentDataset:\n",
    "    def __init__(self, dataset, word_to_index, max_len=30):\n",
    "        self.dataset = dataset\n",
    "        self.word_to_index = word_to_index\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset.iloc[idx]['text']\n",
    "        label = self.dataset.iloc[idx]['label']\n",
    "\n",
    "        # Tokenization and word-to-index conversion\n",
    "        text = text.lower()\n",
    "        word_list = nltk.tokenize.word_tokenize(text)\n",
    "        word_list = [word.strip(\"'\\\"\") for word in word_list]\n",
    "        indices = [self.word_to_index.get(word, self.word_to_index.get('<UNK>')) + 1 for word in word_list]\n",
    "        indices = indices[:self.max_len] + [0] * (self.max_len - len(indices))  # Padding\n",
    "\n",
    "        return np.array(indices), np.array(label)\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        texts = []\n",
    "        labels = []\n",
    "        for i in range(len(self.dataset)):\n",
    "            features, label = self.__getitem__(i)\n",
    "            texts.append(features)\n",
    "            labels.append(label)\n",
    "        return np.array(texts), np.array(labels)\n",
    "\n",
    "#customed dataset\n",
    "class CustomedDataset(Dataset):\n",
    "    def __init__(self,sentences,labels,vocab_word_to_index):\n",
    "        self.features=torch.tensor([[vocab_word_to_index[word] if word in vocab_word_to_index else len(vocab_word_to_index) for word in sentence]+[len(vocab_word_to_index)+1]*(max_len-len(sentence)) for sentence in sentences]).to(device)\n",
    "        self.labels=torch.tensor(labels).to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.features[idx],self.labels[idx]\n",
    "#prepare dataloader\n",
    "def prep_dataloader(train_dataset,validation_dataset,test_dataset,batch_size,vocab_word_to_index):\n",
    "    train_dataloader=DataLoader(CustomedDataset(train_dataset[\"text\"],train_dataset[\"label\"],vocab_word_to_index),batch_size=batch_size,shuffle=True)\n",
    "    validation_dataloader=DataLoader(CustomedDataset(validation_dataset[\"text\"],validation_dataset[\"label\"],vocab_word_to_index),batch_size=batch_size)\n",
    "    test_dataloader=DataLoader(CustomedDataset(test_dataset[\"text\"],test_dataset[\"label\"],vocab_word_to_index),batch_size=batch_size)\n",
    "    return train_dataloader,validation_dataloader,test_dataloader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy from part 2\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, num_layers = 1, freeze_embeddings=False, dropout = 0.5):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "        embedding_matrix=torch.tensor(embedding_matrix,dtype=torch.float32)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=freeze_embeddings)\n",
    "        self.rnn = nn.RNN(50, hidden_dim, num_layers, batch_first=True, device= device)\n",
    "        #self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim, device= device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        out, hidden = self.rnn(embedded)\n",
    "        out = out[:, -1, :]\n",
    "        #out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "#part 3.4 CNN definition\n",
    "class CNNTextClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, n_filters, filter_sizes, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        #embedding from pretrained model\n",
    "        embedding_matrix=torch.tensor(embedding_matrix,dtype=torch.float32)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        #parallel kernels\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv2d(1, n_filters, (fs, embedding_matrix.shape[1])) for fs in filter_sizes]\n",
    "        )\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax=nn.Softmax(-1)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        embedded = self.embedding(sentences)  # embedded = [batch size, sent len, emb dim]\n",
    "        embedded = embedded.unsqueeze(1)  # embedded = [batch size, 1, sent len, emb dim]\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]  # conv_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]  # pooled_n = [batch size, n_filters]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))  # cat = [batch size, n_filters * len(filter_sizes)]\n",
    "        \n",
    "        return self.softmax(self.fc(cat))\n",
    "#copy from 3.3\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_size, num_layers, output_size = 1, model_type = 'lstm'):\n",
    "        super(SentimentModel, self).__init__()\n",
    "        vocab_size, embedding_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype = torch.float32), freeze = False, padding_idx = vocab_size-1)\n",
    "\n",
    "        if model_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers = num_layers, bidirectional = True, batch_first = True)\n",
    "        elif model_type == 'gru':\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_size, num_layers = num_layers, bidirectional = True, batch_first = True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)  # 2 for bidirectional and 1 output class\n",
    "        # self.softmax = nn.Softmax(dim = 1)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Initialize Embedding Layer\n",
    "        nn.init.uniform_(self.embedding.weight, -0.01, 0.01)\n",
    "\n",
    "        # Initialize RNN (LSTM/GRU) weights and biases\n",
    "        for name, param in self.rnn.named_parameters():\n",
    "            if 'weight_ih' in name:  # Input to hidden weights\n",
    "                nn.init.xavier_uniform_(param.data)  # Xavier initialization\n",
    "            elif 'weight_hh' in name:  # Hidden to hidden weights\n",
    "                nn.init.orthogonal_(param.data)  # Orthogonal initialization\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param.data)  # Zero bias initialization\n",
    "\n",
    "        # Initialize Linear (Fully connected) layer\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = torch.eq(x, self.embedding.num_embeddings-1)\n",
    "        lengths= mask.float().argmax(dim=1)-1\n",
    "        print(x)\n",
    "        print(lengths)\n",
    "        embedded = self.embedding(x)\n",
    "        packed_embedded = pack_padded_sequence(embedded, lengths.cpu(), batch_first = True, enforce_sorted = False)\n",
    "        packed_rnn_out, _ = self.rnn(packed_embedded)\n",
    "        rnn_out, _ = pad_packed_sequence(packed_rnn_out, batch_first = True)\n",
    "        final_feature_map = rnn_out[torch.arange(rnn_out.size(0)), lengths - 1]\n",
    "        final_out = self.fc(final_feature_map)\n",
    "        return final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc=nn.Linear(input_dim,output_dim)\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x+self.fc(x))\n",
    "#CNN + res block\n",
    "class CNNTextResidualClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, n_filters, filter_sizes, output_dim, dropout,num_hidden=256,res_block_num=3):\n",
    "        super().__init__()\n",
    "        #embedding from pretrained model\n",
    "        embedding_matrix=torch.tensor(embedding_matrix,dtype=torch.float32)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        #parallel kernels\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv2d(1, n_filters, (fs, embedding_matrix.shape[1])) for fs in filter_sizes]\n",
    "        )\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, num_hidden)\n",
    "        self.relu=nn.ReLU()\n",
    "        #res block\n",
    "        self.res_block=nn.Sequential(*[ResidualBlock(num_hidden,num_hidden) for _ in range(res_block_num)])\n",
    "        self.fc_out=nn.Linear(num_hidden,output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax=nn.Softmax(-1)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        embedded = self.embedding(sentences)  # embedded = [batch size, sent len, emb dim]\n",
    "        embedded = embedded.unsqueeze(1)  # embedded = [batch size, 1, sent len, emb dim]\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]  # conv_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]  # pooled_n = [batch size, n_filters]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))  # cat = [batch size, n_filters * len(filter_sizes)]\n",
    "        res_block_in=self.relu(self.fc(cat))\n",
    "        res_block_out=self.res_block(res_block_in)\n",
    "        return self.softmax(self.fc_out(res_block_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # register parameters\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:,:x.shape[1], :].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "#get key padding mask\n",
    "def get_key_padding_mask(tokens,vocab_size):\n",
    "    key_padding_mask=torch.zeros(tokens.size())\n",
    "    key_padding_mask[tokens==vocab_size-1]=-torch.inf\n",
    "    return key_padding_mask.to(device)\n",
    "#transformer model for part 3.5\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, embedding_matrix, max_len, dropout, num_hidden=64, num_resblock=2, nhead=3, num_encoder_layers=1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.nhead=nhead\n",
    "        #embedding from pretrained model\n",
    "        embedding_matrix=torch.tensor(embedding_matrix,dtype=torch.float32)\n",
    "        d_model=embedding_matrix.shape[1]\n",
    "        self.embedding_src = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        #positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model,max_len=max_len) \n",
    "        #transformer encoder\n",
    "        encoder = nn.TransformerEncoderLayer(d_model=d_model*nhead, nhead=nhead,dim_feedforward=64)\n",
    "        self.transformer = nn.TransformerEncoder(encoder,num_encoder_layers)\n",
    "        #fully connected layers and res blocks\n",
    "        self.fc1 = nn.Linear(d_model*nhead, num_hidden)\n",
    "        self.res_blocks1=nn.Sequential(*[ResidualBlock(num_hidden,num_hidden) for _ in range(num_resblock)])\n",
    "        self.fc2 = nn.Linear(num_hidden,num_hidden)\n",
    "        self.res_blocks2=nn.Sequential(*[ResidualBlock(num_hidden,num_hidden) for _ in range(num_resblock)])\n",
    "        self.fc3 = nn.Linear(num_hidden,2)\n",
    "        self.softmax=nn.Softmax(-1)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src_key_padding_mask = get_key_padding_mask(src,self.embedding_src.num_embeddings)\n",
    "        src_emb = self.positional_encoding(self.embedding_src(src))\n",
    "        \n",
    "        # Transformer forward with attention masks\n",
    "        output = self.transformer(\n",
    "            src_emb.unsqueeze(2).repeat(1,1,self.nhead,1).reshape(src_emb.shape[0],src_emb.shape[1],-1).permute(1, 0, 2), \n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "        ) #[token_num,batch_size,embedding_dim]\n",
    "        \n",
    "        output=self.dropout(self.relu(self.fc1(output.permute(1,0,2))))\n",
    "        output=self.res_blocks1(output)\n",
    "        output=self.dropout(self.relu(self.fc2(torch.max(output,dim=1)[0])))\n",
    "        output=self.res_blocks2(output)\n",
    "        \n",
    "        return self.softmax(self.fc3(output))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model,optimizer,criterion,num_epoch,train_dataloader,validation_dataloader,model_type=\"\"):\n",
    "    from tqdm import tqdm\n",
    "    model.to(device)\n",
    "    for _ in range(num_epoch):\n",
    "        acc_loss=0\n",
    "        model.train()\n",
    "        process_bar=tqdm(train_dataloader,desc=f\"Epoch {_}/{num_epoch}\",leave=True)\n",
    "        for features,labels in process_bar:\n",
    "            \n",
    "            pred=model(features)\n",
    "            optimizer.zero_grad()\n",
    "            if model_type==\"SentimentRNN\":\n",
    "                loss=criterion(pred,labels.unsqueeze(1).float())\n",
    "            else:\n",
    "                loss=criterion(pred,labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            acc_loss+=loss.item()\n",
    "            process_bar.set_postfix_str(f\"Mean loss: {acc_loss/(process_bar.n+1)}\")\n",
    "        \n",
    "        print(\"Train loss:\",acc_loss/process_bar.n)\n",
    "        \n",
    "        acc_loss=0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            acc_loss=0\n",
    "            process_bar=tqdm(validation_dataloader,desc=\"Validating\",leave=True)\n",
    "            for features,labels in process_bar:\n",
    "                \n",
    "                pred=model(features)\n",
    "                \n",
    "                if model_type==\"SentimentRNN\":\n",
    "                    loss=criterion(pred,labels.unsqueeze(1).float())\n",
    "                else:\n",
    "                    loss=criterion(pred,labels)\n",
    "                \n",
    "                acc_loss+=loss.item()\n",
    "                process_bar.set_postfix_str(f\"Mean loss: {acc_loss/(process_bar.n+1)}\")\n",
    "                \n",
    "            print(\"Validation loss:\",acc_loss/process_bar.n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/15: 100%|██████████| 267/267 [00:01<00:00, 173.86it/s, Mean loss: 0.7320256065945381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6936422414547495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 34/34 [00:00<00:00, 351.74it/s, Mean loss: 23.58026432991028]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6935371861738318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 267/267 [00:01<00:00, 189.44it/s, Mean loss: 0.7112458614202646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6925989661770366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 34/34 [00:00<00:00, 411.16it/s, Mean loss: 23.5673810839653]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.69315826717545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|██████████| 267/267 [00:01<00:00, 196.32it/s, Mean loss: 0.6916154585081093]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6916154585081093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 34/34 [00:00<00:00, 367.41it/s, Mean loss: 23.542530477046967]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6924273669719696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|██████████| 267/267 [00:01<00:00, 185.99it/s, Mean loss: 0.6939198076725006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6757271535387647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 34/34 [00:00<00:00, 372.71it/s, Mean loss: 21.495155096054077]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6322104440015905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|██████████| 267/267 [00:01<00:00, 177.51it/s, Mean loss: 0.6183241705099741]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6044292453299748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 34/34 [00:00<00:00, 340.76it/s, Mean loss: 20.919913589954376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6152915761751288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|██████████| 267/267 [00:01<00:00, 173.72it/s, Mean loss: 0.6068818326089896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5796062446265632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 34/34 [00:00<00:00, 382.08it/s, Mean loss: 20.321056246757507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5976781249046326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|██████████| 267/267 [00:01<00:00, 172.90it/s, Mean loss: 0.5932699566773275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5621621686867561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 34/34 [00:00<00:00, 270.13it/s, Mean loss: 0.7067497958030019]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5820292436024722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|██████████| 267/267 [00:01<00:00, 169.73it/s, Mean loss: 0.5548894915940626]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5507330160015977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 34/34 [00:00<00:00, 367.12it/s, Mean loss: 19.607175439596176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5766816305763581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|██████████| 267/267 [00:01<00:00, 166.52it/s, Mean loss: 0.5561243631518804]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5415443236684978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 34/34 [00:00<00:00, 352.89it/s, Mean loss: 19.485952258110046]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5731162428855896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|██████████| 267/267 [00:01<00:00, 172.45it/s, Mean loss: 0.5334873004129317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5334873004129317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 34/34 [00:00<00:00, 327.29it/s, Mean loss: 0.5712831677759395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5712831677759395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|██████████| 267/267 [00:01<00:00, 181.60it/s, Mean loss: 0.5509133006744233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5220264609386859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 34/34 [00:00<00:00, 330.04it/s, Mean loss: 19.222966581583023]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5653813700465595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|██████████| 267/267 [00:01<00:00, 190.08it/s, Mean loss: 0.5301489516977191]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.51227876231465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 34/34 [00:00<00:00, 351.26it/s, Mean loss: 19.376380175352097]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5698935345691793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|██████████| 267/267 [00:01<00:00, 188.20it/s, Mean loss: 0.5231713264947757]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5016174516204591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 34/34 [00:00<00:00, 391.43it/s, Mean loss: 19.61876517534256]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5770225051571342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|██████████| 267/267 [00:01<00:00, 187.31it/s, Mean loss: 0.518279303680174]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4969269728169459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 34/34 [00:00<00:00, 343.39it/s, Mean loss: 19.07141238451004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.56092389366206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|██████████| 267/267 [00:01<00:00, 187.25it/s, Mean loss: 0.5068631423454658] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.48408277639735503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 34/34 [00:00<00:00, 403.97it/s, Mean loss: 20.15124922990799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5926838008796468\n",
      "Test acc is:73.07692307692307%\n"
     ]
    }
   ],
   "source": [
    "def work_flow(model_type,handle_oov,params):\n",
    "    \"\"\"\n",
    "    model_type: str, controls which model to use.\n",
    "    handle_oov: bool, True if implementing oov handling solution discussed in part 1\n",
    "    params: dict, hyper parameters to use\n",
    "    \"\"\"\n",
    "    vocab_word_to_index,embedding_matrix=prep_embedding(handle_oov,model_type)\n",
    "    # prepare dataloaders\n",
    "    if model_type==\"SentimentRNN\":\n",
    "        train_texts, train_labels = SentimentDataset(train_dataset, vocab_word_to_index, max_len=30).preprocess_data()\n",
    "        valid_texts, valid_labels = SentimentDataset(validation_dataset, vocab_word_to_index, max_len=30).preprocess_data()\n",
    "        test_texts, test_labels = SentimentDataset(test_dataset, vocab_word_to_index, max_len=30).preprocess_data()\n",
    "\n",
    "        # Convert preprocessed arrays to PyTorch tensors\n",
    "        train_texts = torch.tensor(train_texts).to(device)\n",
    "        train_labels = torch.tensor(train_labels).to(device)\n",
    "        valid_texts = torch.tensor(valid_texts).to(device)\n",
    "        valid_labels = torch.tensor(valid_labels).to(device)\n",
    "        test_texts = torch.tensor(test_texts).to(device)\n",
    "        test_labels = torch.tensor(test_labels).to(device)\n",
    "\n",
    "        train_datasets = TensorDataset(train_texts, train_labels)\n",
    "        valid_datasets = TensorDataset(valid_texts, valid_labels)\n",
    "        test_datasets = TensorDataset(test_texts, test_labels)\n",
    "\n",
    "        train_dataloader = DataLoader(train_datasets, batch_size=32, shuffle=True)\n",
    "        validation_dataloader = DataLoader(valid_datasets, batch_size=32, shuffle=False)\n",
    "        test_dataloader = DataLoader(test_datasets, batch_size=32, shuffle=False)\n",
    "    else:\n",
    "        train_dataloader,validation_dataloader,test_dataloader=prep_dataloader(train_dataset,validation_dataset,test_dataset,params[\"batch_size\"],vocab_word_to_index)\n",
    "\n",
    "    #instantiating models\n",
    "    if model_type==\"CNN\":\n",
    "        model = CNNTextClassifier(embedding_matrix, params[\"n_filters\"], params[\"filter_sizes\"], params[\"output_dim\"], params[\"dropout\"])\n",
    "    if model_type==\"CNN_res_block\":\n",
    "        model = CNNTextResidualClassifier(embedding_matrix, params[\"n_filters\"], params[\"filter_sizes\"], params[\"output_dim\"], params[\"dropout\"])\n",
    "    if model_type==\"transformer\":\n",
    "        model = TransformerModel(embedding_matrix,max_len,params[\"dropout\"])\n",
    "    if model_type==\"SentimentRNN\":\n",
    "        model = SentimentRNN(embedding_matrix,params[\"hidden_dim\"],params[\"output_dim\"])\n",
    "    if model_type==\"SentimentModel\":\n",
    "        model = SentimentModel(embedding_matrix,params[\"hidden_size\"],params[\"num_layers\"])\n",
    "    #preparing loss function and optimizer\n",
    "    criterion=nn.CrossEntropyLoss()\n",
    "    if model_type==\"SentimentRNN\":\n",
    "        criterion=nn.BCELoss()\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=params[\"lr\"])\n",
    "    \n",
    "    train(model,optimizer,criterion,params[\"num_epoch\"],train_dataloader,validation_dataloader,model_type)\n",
    "    #get the test accuracy\n",
    "    model.eval()\n",
    "    test_acc=0\n",
    "    tot_samples=0\n",
    "    with torch.no_grad():\n",
    "        for features,labels in test_dataloader:\n",
    "            pred_labels=model(features)\n",
    "            # count number of correct predictions\n",
    "            if model_type==\"SentimentRNN\":\n",
    "                test_acc+=(labels==(pred_labels>0.5).int().squeeze()).sum().item()\n",
    "            else:\n",
    "                test_acc+=(labels==pred_labels.argmax(1)).sum().item()\n",
    "            tot_samples+=labels.shape[0]\n",
    "        print(f\"Test acc is:{test_acc/tot_samples*100}%\")\n",
    "\n",
    "params={\"batch_size\":32,\"n_filters\":32,\"filter_sizes\":[1,2,3,5],\"hidden_dim\":128,\"output_dim\":1,\"dropout\":0.1,\"lr\":0.00005,\"hidden_size\":64,\"num_layers\":3,\"num_epoch\":15}\n",
    "work_flow(\"SentimentRNN\",True,params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
