{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train'].to_pandas()\n",
    "validation_dataset = dataset['validation'].to_pandas()\n",
    "test_dataset = dataset['test'].to_pandas()\n",
    "max_len=max(0,train_dataset[\"text\"].apply(lambda x:len(x)).max())\n",
    "max_len=max(max_len,validation_dataset[\"text\"].apply(lambda x:len(x)).max())\n",
    "max_len=max(max_len,test_dataset[\"text\"].apply(lambda x:len(x)).max())\n",
    "max_len+=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def prep_pretrained_embedding():\n",
    "    def build_vocab(train_dataset):\n",
    "        # Create set, unique words only\n",
    "        vocab = set()\n",
    "        train_dataset_pos = []\n",
    "        \n",
    "        # Loop thru each sentence in training dataset\n",
    "        for sentence in train_dataset['text']:\n",
    "            # Basic text processing\n",
    "            \n",
    "            # Case folding\n",
    "            sentence = sentence.lower()\n",
    "            \n",
    "            # NLTK tokenizer does a good job at separating meaningful words + punctuations\n",
    "            # Better than defining regex ourselves\n",
    "            word_list = nltk.tokenize.word_tokenize(sentence)\n",
    "            \n",
    "            # # Further split words into separate words\n",
    "            # # e.g., 'well-being' -> 'well', 'being'\n",
    "            # # e.g., 'music/song' -> 'music', 'song'\n",
    "            # split_word_list = []\n",
    "            # for word in sentence_list:\n",
    "            #     split_word_list.extend(word.replace('-', ' ').replace('/', ' ').split())\n",
    "            \n",
    "            # Dont remove all special characters, some are meaningful\n",
    "            # Some words are surrounded by single/double quotes\n",
    "            word_list = [word.strip(\"'\\\"\") for word in word_list]\n",
    "            \n",
    "            # Add into set\n",
    "            vocab.update(word_list)\n",
    "            \n",
    "            # Get pos tags\n",
    "            # Also build POS tags\n",
    "            pos_tags = nltk.pos_tag(word_list)\n",
    "            train_dataset_pos.append(pos_tags)\n",
    "            \n",
    "        vocab.discard('')\n",
    "        return vocab, train_dataset_pos\n",
    "\n",
    "    vocab, train_dataset_pos = build_vocab(train_dataset)\n",
    "\n",
    "\n",
    "\n",
    "    def load_glove_embeddings(path):\n",
    "        glove_embeddings = {}\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype='float64')\n",
    "                glove_embeddings[word] = vector\n",
    "                \n",
    "        return glove_embeddings\n",
    "\n",
    "    glove_embeddings = load_glove_embeddings('glove.6B.50d.txt')\n",
    "    vocab_word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "    def create_embedding_matrix(word_to_index, glove_embeddings):\n",
    "        # Initialize embedding matrix with zeros\n",
    "        # 50d\n",
    "        embedding_matrix = np.zeros((len(vocab)+2, 50), dtype='float64')\n",
    "        \n",
    "        # Loop thru each word in vocab\n",
    "        for word, idx in word_to_index.items():\n",
    "            # Check if word exists in glove embeddings\n",
    "            if word in glove_embeddings:\n",
    "                # Copy glove embedding to embedding matrix\n",
    "                embedding_matrix[idx] = glove_embeddings[word]\n",
    "                # If OOV, assign None first\n",
    "                \n",
    "        return embedding_matrix\n",
    "\n",
    "    embedding_matrix = create_embedding_matrix(vocab_word_to_index, glove_embeddings)\n",
    "    #handle <unk>\n",
    "    embedding_matrix[-2]=[ 0.01513297,  0.2400952 , -0.13676383,  0.13166569, -0.28283166,\n",
    "        0.10421129,  0.39747017,  0.07944959,  0.29670785,  0.05400998,\n",
    "        0.48425894,  0.26516231, -0.48021244, -0.25129253, -0.24367068,\n",
    "       -0.24188322,  0.47579495, -0.2097357 , -0.02568224, -0.31143999,\n",
    "       -0.3196337 ,  0.44878632, -0.07379564,  0.32765833, -0.49052161,\n",
    "       -0.33455611, -0.34772199, -0.05043562, -0.0898296 ,  0.04898804,\n",
    "        0.4993778 ,  0.04359836,  0.40077601, -0.31343237,  0.24126281,\n",
    "       -0.4907152 , -0.20372591, -0.32123346, -0.39554707,  0.37386547,\n",
    "        0.44720326,  0.45492689, -0.16420979,  0.42844699,  0.15748723,\n",
    "       -0.23547929, -0.33962153,  0.04243802, -0.03647524, -0.0042893 ]\n",
    "    \n",
    "    return vocab_word_to_index,embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def prep_embedding(handle_oov=False):\n",
    "    if handle_oov:\n",
    "        with open('embedding_matrix.pkl', 'rb') as file:  \n",
    "            embedding_matrix = pickle.load(file)\n",
    "            embedding_matrix = np.concatenate((embedding_matrix, np.zeros((1, 50))), axis=0)\n",
    "        with open('vocab_word_to_index.pkl', 'rb') as file:  \n",
    "            vocab_word_to_index = pickle.load(file)\n",
    "            del vocab_word_to_index['<UNK>']\n",
    "    else:\n",
    "        vocab_word_to_index,embedding_matrix= prep_pretrained_embedding()\n",
    "        embedding_matrix[-1]=np.zeros(50)\n",
    "    # print(embedding_matrix)\n",
    "    # print(embedding_matrix.shape)\n",
    "    # print(len(vocab_word_to_index))\n",
    "    # print(vocab_word_to_index)\n",
    "    return vocab_word_to_index,embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "device=torch.device('cuda')\n",
    "\n",
    "class CustomedDataset(Dataset):\n",
    "    def __init__(self,sentences,labels,vocab_word_to_index):\n",
    "        self.features=torch.tensor([[vocab_word_to_index[word] if word in vocab_word_to_index else len(vocab_word_to_index) for word in sentence]+[len(vocab_word_to_index)+1]*(max_len-len(sentence)) for sentence in sentences]).to(device)\n",
    "        self.labels=torch.tensor(labels).to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.features[idx],self.labels[idx]\n",
    "\n",
    "def prep_dataloader(train_dataset,validation_dataset,test_dataset,batch_size,vocab_word_to_index):\n",
    "    train_dataloader=DataLoader(CustomedDataset(train_dataset[\"text\"],train_dataset[\"label\"],vocab_word_to_index),batch_size=batch_size,shuffle=True)\n",
    "    validation_dataloader=DataLoader(CustomedDataset(validation_dataset[\"text\"],validation_dataset[\"label\"],vocab_word_to_index),batch_size=batch_size)\n",
    "    test_dataloader=DataLoader(CustomedDataset(test_dataset[\"text\"],test_dataset[\"label\"],vocab_word_to_index),batch_size=batch_size)\n",
    "    return train_dataloader,validation_dataloader,test_dataloader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CNNTextClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, n_filters, filter_sizes, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        embedding_matrix=torch.tensor(embedding_matrix,dtype=torch.float32)\n",
    "        # print(embedding_matrix)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        # self.embedding = embedding_matrix\n",
    "        # print(self.embedding.shape)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv2d(1, n_filters, (fs, embedding_matrix.shape[1])) for fs in filter_sizes]\n",
    "        )\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax=nn.Softmax(-1)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        # text = [batch size, sent len]\n",
    "        # embedded=[[self.embedding[idx] for idx in sentence] for sentence in sentences]\n",
    "        embedded = self.embedding(sentences)  # embedded = [batch size, sent len, emb dim]\n",
    "        embedded = embedded.unsqueeze(1)  # embedded = [batch size, 1, sent len, emb dim]\n",
    "        # print(embedded)\n",
    "        # print(embedded.shape)\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]  # conv_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]  # pooled_n = [batch size, n_filters]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))  # cat = [batch size, n_filters * len(filter_sizes)]\n",
    "        \n",
    "        return self.softmax(self.fc(cat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc=nn.Linear(input_dim,output_dim)\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x+self.fc(x))\n",
    "\n",
    "class CNNTextResidualClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, n_filters, filter_sizes, output_dim, dropout,num_hidden=256,res_block_num=3):\n",
    "        super().__init__()\n",
    "        embedding_matrix=torch.tensor(embedding_matrix,dtype=torch.float32)\n",
    "        # print(embedding_matrix)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        # self.embedding = embedding_matrix\n",
    "        # print(self.embedding.shape)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv2d(1, n_filters, (fs, embedding_matrix.shape[1])) for fs in filter_sizes]\n",
    "        )\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, num_hidden)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.res_block=nn.Sequential(*[ResidualBlock(num_hidden,num_hidden) for _ in range(res_block_num)])\n",
    "        self.fc_out=nn.Linear(num_hidden,output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax=nn.Softmax(-1)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        # text = [batch size, sent len]\n",
    "        # embedded=[[self.embedding[idx] for idx in sentence] for sentence in sentences]\n",
    "        embedded = self.embedding(sentences)  # embedded = [batch size, sent len, emb dim]\n",
    "        embedded = embedded.unsqueeze(1)  # embedded = [batch size, 1, sent len, emb dim]\n",
    "        # print(embedded)\n",
    "        # print(embedded.shape)\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]  # conv_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]  # pooled_n = [batch size, n_filters]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))  # cat = [batch size, n_filters * len(filter_sizes)]\n",
    "        res_block_in=self.relu(self.fc(cat))\n",
    "        res_block_out=self.res_block(res_block_in)\n",
    "        return self.softmax(self.fc_out(res_block_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # 创建一个大的位置编码矩阵\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # 将位置编码作为常量注册到模型中，不需要梯度更新\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        # print(self.pe.shape)\n",
    "        x = x + self.pe[:,:x.shape[1], :].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "def get_key_padding_mask(tokens,vocab_size):\n",
    "    key_padding_mask=torch.zeros(tokens.size())\n",
    "    key_padding_mask[tokens==vocab_size-1]=-torch.inf\n",
    "    return key_padding_mask.to(device)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, embedding_matrix, max_len, dropout, num_hidden=64, num_resblock=3, nhead=5, num_encoder_layers=2):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        embedding_matrix=torch.tensor(embedding_matrix,dtype=torch.float32)\n",
    "        d_model=embedding_matrix.shape[1]\n",
    "        self.embedding_src = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.positional_encoding = PositionalEncoding(d_model,max_len=max_len)  # 最大序列长度为1650\n",
    "        encoder = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,dim_feedforward=128)\n",
    "        self.transformer = nn.TransformerEncoder(encoder,num_encoder_layers)\n",
    "        self.fc1 = nn.Linear(d_model, num_hidden)\n",
    "        self.res_blocks1=nn.Sequential(*[ResidualBlock(num_hidden,num_hidden) for _ in range(num_resblock)])\n",
    "        self.fc2 = nn.Linear(num_hidden*max_len,num_hidden)\n",
    "        self.res_blocks2=nn.Sequential(*[ResidualBlock(num_hidden,num_hidden) for _ in range(num_resblock)])\n",
    "        self.fc3 = nn.Linear(num_hidden,2)\n",
    "        self.softmax=nn.Softmax(-1)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src_key_padding_mask = get_key_padding_mask(src,self.embedding_src.num_embeddings)\n",
    "        src_emb = self.positional_encoding(self.embedding_src(src)*torch.sqrt(torch.tensor(self.embedding_src.embedding_dim, dtype=torch.float32)))\n",
    "        # Transformer forward with attention masks\n",
    "        output = self.transformer(\n",
    "            src_emb.permute(1, 0, 2), \n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "        ) #[token_num,batch_size,embedding_dim]\n",
    "        # pooled_output=nn.MaxPool2d(,)\n",
    "        output=self.dropout(self.relu(self.fc1(output.permute(1,0,2))))\n",
    "        output=self.res_blocks1(output)\n",
    "        output=self.dropout(self.relu(self.fc2(output.reshape(output.shape[0],-1))))\n",
    "        output=self.res_blocks2(output)\n",
    "        return self.softmax(self.fc3(output))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model,optimizer,criterion,num_epoch,train_dataloader,validation_dataloader):\n",
    "    from tqdm import tqdm\n",
    "    model.to(device)\n",
    "    for _ in range(num_epoch):\n",
    "        acc_loss=0\n",
    "        model.train()\n",
    "        process_bar=tqdm(train_dataloader,desc=f\"Epoch {_}/{num_epoch}\",leave=True)\n",
    "        for features,labels in process_bar:\n",
    "            \n",
    "            pred=model(features)\n",
    "            # print(pred,pred.shape)\n",
    "            # print(labels,labels.shape)\n",
    "            optimizer.zero_grad()\n",
    "            loss=criterion(pred,labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            acc_loss+=loss.item()\n",
    "            process_bar.set_postfix_str(f\"Mean loss: {acc_loss/(process_bar.n+1)}\")\n",
    "        \n",
    "        print(\"Train loss:\",acc_loss/process_bar.n)\n",
    "        \n",
    "        acc_loss=0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            acc_loss=0\n",
    "            process_bar=tqdm(validation_dataloader,desc=\"Validating\",leave=True)\n",
    "            for features,labels in process_bar:\n",
    "                \n",
    "                pred=model(features)\n",
    "                \n",
    "                loss=criterion(pred,labels)\n",
    "                \n",
    "                acc_loss+=loss.item()\n",
    "                process_bar.set_postfix_str(f\"Mean loss: {acc_loss/(process_bar.n+1)}\")\n",
    "                \n",
    "            print(\"Validation loss:\",acc_loss/process_bar.n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/10: 100%|██████████| 34/34 [00:45<00:00,  1.35s/it, Mean loss: 0.6972135673550999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6972135673550999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00,  6.71it/s, Mean loss: 0.691911256313324] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.691911256313324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 34/34 [00:48<00:00,  1.43s/it, Mean loss: 0.6935540648067698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6935540648067698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:01<00:00,  3.96it/s, Mean loss: 0.6913015604019165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6913015604019165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 34/34 [00:45<00:00,  1.33s/it, Mean loss: 0.6931360258775598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6931360258775598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00, 11.13it/s, Mean loss: 0.6892780542373658]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6892780542373658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 34/34 [00:27<00:00,  1.23it/s, Mean loss: 0.6908555206130532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6908555206130532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00,  6.41it/s, Mean loss: 0.6819021940231323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6819021940231323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 34/34 [00:38<00:00,  1.14s/it, Mean loss: 0.6854179589187398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6854179589187398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00,  7.21it/s, Mean loss: 0.6750262379646301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6750262379646301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 34/34 [00:34<00:00,  1.02s/it, Mean loss: 0.6845513003713944]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6845513003713944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00,  6.38it/s, Mean loss: 0.6825941324234008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6825941324234008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 34/34 [00:41<00:00,  1.21s/it, Mean loss: 0.6761371160254759]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6761371160254759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00,  6.95it/s, Mean loss: 0.6957069039344788]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6957069039344788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 34/34 [00:47<00:00,  1.39s/it, Mean loss: 0.6745475407908944]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6745475407908944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00,  6.44it/s, Mean loss: 0.6814854383468628]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6814854383468628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 34/34 [00:34<00:00,  1.01s/it, Mean loss: 0.6640392489293042]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6640392489293042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00,  8.93it/s, Mean loss: 0.7158849596977234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7158849596977234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 34/34 [00:26<00:00,  1.28it/s, Mean loss: 0.6571861365262199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6571861365262199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 5/5 [00:00<00:00,  9.02it/s, Mean loss: 0.868243858218193] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6945950865745545\n",
      "Test acc is:54.87804878048781%\n"
     ]
    }
   ],
   "source": [
    "def work_flow(model_type,handle_oov,params):\n",
    "    vocab_word_to_index,embedding_matrix=prep_embedding(handle_oov)\n",
    "    # print(embedding_matrix)\n",
    "    train_dataloader,validation_dataloader,test_dataloader=prep_dataloader(train_dataset,validation_dataset,test_dataset,params[\"batch_size\"],vocab_word_to_index)\n",
    "\n",
    "    if model_type==\"CNN\":\n",
    "        model = CNNTextClassifier(embedding_matrix, params[\"n_filters\"], params[\"filter_sizes\"], params[\"output_dim\"], params[\"dropout\"])\n",
    "    if model_type==\"CNN_res_block\":\n",
    "        model = CNNTextResidualClassifier(embedding_matrix, params[\"n_filters\"], params[\"filter_sizes\"], params[\"output_dim\"], params[\"dropout\"])\n",
    "    if model_type==\"transformer\":\n",
    "        model = TransformerModel(embedding_matrix,max_len,params[\"dropout\"])\n",
    "\n",
    "    criterion=nn.CrossEntropyLoss()\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=params[\"lr\"])\n",
    "    \n",
    "    train(model,optimizer,criterion,params[\"num_epoch\"],train_dataloader,validation_dataloader)\n",
    "    \n",
    "    model.eval()\n",
    "    test_acc=0\n",
    "    tot_samples=0\n",
    "    with torch.no_grad():\n",
    "        for features,labels in test_dataloader:\n",
    "            pred_labels=model(features)\n",
    "            # print(labels.shape,pred_labels.shape)\n",
    "            test_acc+=(labels==pred_labels.argmax(dim=1)).sum().item()\n",
    "            tot_samples+=labels.shape[0]\n",
    "        print(f\"Test acc is:{test_acc/tot_samples*100}%\")\n",
    "\n",
    "params={\"batch_size\":256,\"n_filters\":32,\"filter_sizes\":[1,2,3,5],\"output_dim\":2,\"dropout\":0.2,\"lr\":0.0002,\"num_epoch\":10}\n",
    "work_flow(\"transformer\",True,params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
