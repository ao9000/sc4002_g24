{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 0 & 1",
   "id": "d132c3228be3545e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load dataset",
   "id": "574c9b9f5af88309"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-14T08:12:39.575407Z",
     "start_time": "2024-10-14T08:12:30.707620Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset ('rotten_tomatoes')\n",
    "train_dataset = dataset ['train']\n",
    "validation_dataset = dataset ['validation']\n",
    "test_dataset = dataset ['test']"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aozy\\anaconda3\\envs\\sc4002\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T08:12:39.607667Z",
     "start_time": "2024-10-14T08:12:39.587405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df = train_dataset.to_pandas()\n",
    "train_df.head()"
   ],
   "id": "3d4675c2fa3aff25",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                text  label\n",
       "0  the rock is destined to be the 21st century's ...      1\n",
       "1  the gorgeously elaborate continuation of \" the...      1\n",
       "2                     effective but too-tepid biopic      1\n",
       "3  if you sometimes like to go to the movies to h...      1\n",
       "4  emerges as something rare , an issue movie tha...      1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the rock is destined to be the 21st century's ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the gorgeously elaborate continuation of \" the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>effective but too-tepid biopic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emerges as something rare , an issue movie tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create vocab from training dataset",
   "id": "fe1d31a2d6c77f11"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T08:16:36.793384Z",
     "start_time": "2024-10-14T08:16:36.788995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import string\n",
    "\n",
    "def build_vocab(train_dataset):\n",
    "    # Create set, unique words only\n",
    "    vocab = set()\n",
    "    # Loop thru each sentence in training dataset\n",
    "    for sentence in train_dataset['text']:\n",
    "        # Add into set\n",
    "        vocab.update(sentence.split())\n",
    "            \n",
    "    vocab.discard('')\n",
    "    return vocab"
   ],
   "id": "c16ba12c57ac418d",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T08:16:37.282122Z",
     "start_time": "2024-10-14T08:16:37.250018Z"
    }
   },
   "cell_type": "code",
   "source": "vocab = build_vocab(train_dataset)",
   "id": "6b6818caab4022aa",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## (a) What is the size of the vocabulary formed from your training data?",
   "id": "2625ddd84fa100cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T08:16:57.072879Z",
     "start_time": "2024-10-14T08:16:57.068537Z"
    }
   },
   "cell_type": "code",
   "source": "print(f'Vocab size/Unique words: {len(vocab)}')",
   "id": "99ab5d75e45c8ae9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size/Unique words: 18951\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 1: Preparing Word Embeddings",
   "id": "5d2372bc84ffd0d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Download GloVe embeddings: https://nlp.stanford.edu/projects/glove/\n",
    "- Uncased means all words are lowercase"
   ],
   "id": "2935a1e76bf50831"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T08:16:59.014270Z",
     "start_time": "2024-10-14T08:16:59.009060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load GloVe embeddings\n",
    "import numpy as np\n",
    "\n",
    "def load_glove_embeddings(path):\n",
    "    glove_embeddings = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float64')\n",
    "            glove_embeddings[word] = vector\n",
    "            \n",
    "    return glove_embeddings"
   ],
   "id": "c2d975ebdc42a9e3",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T08:17:03.982073Z",
     "start_time": "2024-10-14T08:16:59.842857Z"
    }
   },
   "cell_type": "code",
   "source": "glove_embeddings = load_glove_embeddings('glove.6B.50d.txt')",
   "id": "1a16e8ff84207e34",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T08:17:03.997076Z",
     "start_time": "2024-10-14T08:17:03.992073Z"
    }
   },
   "cell_type": "code",
   "source": "print(f'Glove embedding matrix for \"the\":\\n {glove_embeddings[\"the\"]}')",
   "id": "567a91c4cde99338",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove embedding matrix for \"the\":\n",
      " [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T08:17:04.237113Z",
     "start_time": "2024-10-14T08:17:04.233024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print size of the matrix\n",
    "print(f'Number of unique words in embedding matrix: {len(glove_embeddings)}')\n",
    "\n",
    "print(f'Number of Dimension/Features of Glove embedding matrix: {glove_embeddings[\"the\"].shape[0]}')"
   ],
   "id": "b11dddcc16b14aa5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in embedding matrix: 400000\n",
      "Number of Dimension/Features of Glove embedding matrix: 50\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## (b) We use OOV (out-of-vocabulary) to refer to those words appeared in the training data but not in the Word2vec (or Glove) dictionary. How many OOV words exist in your training data?",
   "id": "524c7aeafe09dec7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Method 1: Check difference directly",
   "id": "4f761a07a1afcbc9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T08:21:50.883346Z",
     "start_time": "2024-10-14T08:21:50.879940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_oov_words(vocab, glove_embeddings):\n",
    "    oov_words = []\n",
    "    \n",
    "    for word in vocab:\n",
    "        if word not in glove_embeddings:\n",
    "            oov_words.append(word)\n",
    "            \n",
    "    return oov_words"
   ],
   "id": "e35a81a2f6149f61",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T08:21:51.120353Z",
     "start_time": "2024-10-14T08:21:51.109276Z"
    }
   },
   "cell_type": "code",
   "source": "oov_words = get_oov_words(vocab, glove_embeddings)",
   "id": "70395204463decbc",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T08:21:51.378506Z",
     "start_time": "2024-10-14T08:21:51.372278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "oov_words.sort()\n",
    "oov_words[:100]"
   ],
   "id": "8fe586991fdaa63a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#3',\n",
       " '#9',\n",
       " '$1',\n",
       " '$100',\n",
       " '$20',\n",
       " '$40',\n",
       " '$50-million',\n",
       " '$7',\n",
       " '$9',\n",
       " '$99',\n",
       " \"''independent\",\n",
       " \"'50's\",\n",
       " \"'60s-homage\",\n",
       " \"'70's\",\n",
       " \"'[hopkins]doesn't\",\n",
       " \"'[the\",\n",
       " \"'a\",\n",
       " \"'a'\",\n",
       " \"'abandon\",\n",
       " \"'ace\",\n",
       " \"'action\",\n",
       " \"'ah\",\n",
       " \"'alabama'\",\n",
       " \"'all\",\n",
       " \"'alternate\",\n",
       " \"'amateur'\",\n",
       " \"'analyze\",\n",
       " \"'angels\",\n",
       " \"'anyone\",\n",
       " \"'are\",\n",
       " \"'artistically'\",\n",
       " \"'artístico'\",\n",
       " \"'assassin'\",\n",
       " \"'aunque\",\n",
       " \"'b'\",\n",
       " \"'bad'\",\n",
       " \"'baran\",\n",
       " \"'barbershop\",\n",
       " \"'barbershop'\",\n",
       " \"'bartleby'\",\n",
       " \"'been\",\n",
       " \"'belgium's\",\n",
       " \"'best\",\n",
       " \"'blade\",\n",
       " \"'blood'\",\n",
       " \"'blue\",\n",
       " \"'blundering'\",\n",
       " \"'bold'\",\n",
       " \"'bowling\",\n",
       " \"'brazil\",\n",
       " \"'butterfingered'\",\n",
       " \"'carente\",\n",
       " \"'challenging'\",\n",
       " \"'chan\",\n",
       " \"'charly'\",\n",
       " \"'chick\",\n",
       " \"'children's'\",\n",
       " \"'chops'\",\n",
       " \"'christian\",\n",
       " \"'classic\",\n",
       " \"'classic'\",\n",
       " \"'co-stars\",\n",
       " \"'comedian'\",\n",
       " \"'comedy\",\n",
       " \"'compleja\",\n",
       " \"'cq\",\n",
       " \"'credit'\",\n",
       " \"'cultural\",\n",
       " \"'date\",\n",
       " \"'de\",\n",
       " \"'deadly\",\n",
       " \"'difficult'\",\n",
       " \"'divertida\",\n",
       " \"'do\",\n",
       " \"'dog'\",\n",
       " \"'dragonfly'\",\n",
       " \"'drama\",\n",
       " \"'drumline'\",\n",
       " \"'dumb\",\n",
       " \"'e'\",\n",
       " \"'easier'\",\n",
       " \"'easily\",\n",
       " \"'edgy\",\n",
       " \"'ejemplo\",\n",
       " \"'empowerment\",\n",
       " \"'enigma'\",\n",
       " \"'epic\",\n",
       " \"'estupendamente\",\n",
       " \"'evelyn\",\n",
       " \"'face\",\n",
       " \"'fatal\",\n",
       " \"'film\",\n",
       " \"'fish\",\n",
       " \"'frailty\",\n",
       " \"'frankly\",\n",
       " \"'funny\",\n",
       " \"'garth'\",\n",
       " \"'german-expressionist\",\n",
       " \"'get\",\n",
       " \"'girls\"]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T08:21:53.479583Z",
     "start_time": "2024-10-14T08:21:53.474441Z"
    }
   },
   "cell_type": "code",
   "source": "len(oov_words)",
   "id": "90cea11e0a09793a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3036"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Method 2: Train tokenizer and check OOV words",
   "id": "6fc526ebddf6c322"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T08:21:54.822544Z",
     "start_time": "2024-10-14T08:21:54.670937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_dataset['text'])"
   ],
   "id": "893f1a77a3369944",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T08:21:55.290112Z",
     "start_time": "2024-10-14T08:21:55.286172Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Number of unique words in dictionary= {len(tokenizer.word_index)}\")",
   "id": "86db77373d0170a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in dictionary= 17451\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T08:21:55.624076Z",
     "start_time": "2024-10-14T08:21:55.611262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "oov_words = []\n",
    "for word in tokenizer.word_index:\n",
    "    if word not in glove_embeddings:\n",
    "        oov_words.append(word)"
   ],
   "id": "d0d1e26be30c4f01",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T08:21:56.254944Z",
     "start_time": "2024-10-14T08:21:56.247963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "oov_words.sort()\n",
    "oov_words[:100]"
   ],
   "id": "caa31b435b00ee52",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"''independent\",\n",
       " \"'50's\",\n",
       " \"'70's\",\n",
       " \"'a\",\n",
       " \"'a'\",\n",
       " \"'abandon\",\n",
       " \"'ace\",\n",
       " \"'action\",\n",
       " \"'ah\",\n",
       " \"'alabama'\",\n",
       " \"'all\",\n",
       " \"'alternate\",\n",
       " \"'amateur'\",\n",
       " \"'analyze\",\n",
       " \"'angels\",\n",
       " \"'anyone\",\n",
       " \"'are\",\n",
       " \"'artistically'\",\n",
       " \"'artístico'\",\n",
       " \"'assassin'\",\n",
       " \"'aunque\",\n",
       " \"'b'\",\n",
       " \"'bad'\",\n",
       " \"'baran\",\n",
       " \"'barbershop\",\n",
       " \"'barbershop'\",\n",
       " \"'bartleby'\",\n",
       " \"'been\",\n",
       " \"'belgium's\",\n",
       " \"'best\",\n",
       " \"'blade\",\n",
       " \"'blood'\",\n",
       " \"'blue\",\n",
       " \"'blundering'\",\n",
       " \"'bold'\",\n",
       " \"'bowling\",\n",
       " \"'brazil\",\n",
       " \"'butterfingered'\",\n",
       " \"'carente\",\n",
       " \"'challenging'\",\n",
       " \"'chan\",\n",
       " \"'charly'\",\n",
       " \"'chick\",\n",
       " \"'children's'\",\n",
       " \"'chops'\",\n",
       " \"'christian\",\n",
       " \"'classic\",\n",
       " \"'classic'\",\n",
       " \"'co\",\n",
       " \"'comedian'\",\n",
       " \"'comedy\",\n",
       " \"'compleja\",\n",
       " \"'cq\",\n",
       " \"'credit'\",\n",
       " \"'cultural\",\n",
       " \"'date\",\n",
       " \"'de\",\n",
       " \"'deadly\",\n",
       " \"'difficult'\",\n",
       " \"'divertida\",\n",
       " \"'do\",\n",
       " \"'dog'\",\n",
       " \"'dragonfly'\",\n",
       " \"'drama\",\n",
       " \"'drumline'\",\n",
       " \"'dumb\",\n",
       " \"'e'\",\n",
       " \"'easier'\",\n",
       " \"'easily\",\n",
       " \"'edgy\",\n",
       " \"'ejemplo\",\n",
       " \"'empowerment\",\n",
       " \"'enigma'\",\n",
       " \"'epic\",\n",
       " \"'estupendamente\",\n",
       " \"'evelyn\",\n",
       " \"'face\",\n",
       " \"'fatal\",\n",
       " \"'film\",\n",
       " \"'fish\",\n",
       " \"'frailty\",\n",
       " \"'frankly\",\n",
       " \"'funny\",\n",
       " \"'garth'\",\n",
       " \"'german\",\n",
       " \"'get\",\n",
       " \"'girls\",\n",
       " \"'god\",\n",
       " \"'grandeur'\",\n",
       " \"'great\",\n",
       " \"'guests\",\n",
       " \"'guy's\",\n",
       " \"'hannibal'\",\n",
       " \"'have\",\n",
       " \"'hey\",\n",
       " \"'home\",\n",
       " \"'hosts'\",\n",
       " \"'how\",\n",
       " \"'hungry\",\n",
       " \"'i\"]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T08:21:57.578918Z",
     "start_time": "2024-10-14T08:21:57.575019Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Number of OOV words= {len(oov_words)}\")",
   "id": "56c16b1303634e8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OOV words= 1760\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## (c) The existence of the OOV words is one of the well-known limitations of Word2vec (or Glove). Without using any transformer-based language models (e.g., BERT, GPT, T5), what do you think is the best strategy to mitigate such limitation? Implement your solution in your source code. Show the corresponding code snippet.",
   "id": "fa2c21ef91a8c4db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Most of the OOV words consists of punctuations, numbers, and special characters.",
   "id": "43615d7b46f4e248"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e6837be80c2429ec"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
